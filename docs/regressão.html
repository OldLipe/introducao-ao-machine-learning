<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Regressão | Introdução ao Machine Learning</title>
  <meta name="description" content="Livro para alunos e alunas que querem iniciar em Machine Learning." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Regressão | Introdução ao Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://dataat.github.io/introducao-ao-machine-learning/" />
  <meta property="og:image" content="https://dataat.github.io/introducao-ao-machine-learning/assets/capa.png" />
  <meta property="og:description" content="Livro para alunos e alunas que querem iniciar em Machine Learning." />
  <meta name="github-repo" content="dataat/introducao-ao-machine-learning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Regressão | Introdução ao Machine Learning" />
  
  <meta name="twitter:description" content="Livro para alunos e alunas que querem iniciar em Machine Learning." />
  <meta name="twitter:image" content="https://dataat.github.io/introducao-ao-machine-learning/assets/capa.png" />

<meta name="author" content="Adriano Almeida" />
<meta name="author" content="Felipe Carvalho" />
<meta name="author" content="Felipe Menino" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introdução.html"/>
<link rel="next" href="classificação.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#grupo-dataat"><i class="fa fa-check"></i>Grupo DataAt</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#nossos-livros-textos"><i class="fa fa-check"></i>Nossos livros-textos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#licença"><i class="fa fa-check"></i>Licença</a></li>
</ul></li>
<li class="part"><span><b>I Introdução</b></span></li>
<li class="chapter" data-level="1" data-path="introdução.html"><a href="introdução.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="introdução.html"><a href="introdução.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introdução.html"><a href="introdução.html#aprendizado-supervisionado"><i class="fa fa-check"></i><b>1.1.1</b> Aprendizado supervisionado</a></li>
<li class="chapter" data-level="1.1.2" data-path="introdução.html"><a href="introdução.html#aprendizado-não-supervisionado"><i class="fa fa-check"></i><b>1.1.2</b> Aprendizado Não supervisionado</a></li>
<li class="chapter" data-level="1.1.3" data-path="introdução.html"><a href="introdução.html#aprendizado-por-reforço"><i class="fa fa-check"></i><b>1.1.3</b> Aprendizado por reforço</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Aprendizado Supervisionado</b></span></li>
<li class="chapter" data-level="2" data-path="regressão.html"><a href="regressão.html"><i class="fa fa-check"></i><b>2</b> Regressão</a><ul>
<li class="chapter" data-level="2.1" data-path="regressão.html"><a href="regressão.html#regressão-linear"><i class="fa fa-check"></i><b>2.1</b> Regressão Linear</a><ul>
<li class="chapter" data-level="2.1.1" data-path="regressão.html"><a href="regressão.html#coeficientes-da-regressão-linear"><i class="fa fa-check"></i><b>2.1.1</b> Coeficientes da regressão linear</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="regressão.html"><a href="regressão.html#máquina-de-vetores-de-suporte"><i class="fa fa-check"></i><b>2.2</b> Máquina de vetores de suporte</a><ul>
<li class="chapter" data-level="2.2.1" data-path="regressão.html"><a href="regressão.html#kernels"><i class="fa fa-check"></i><b>2.2.1</b> <em>Kernels</em></a></li>
<li class="chapter" data-level="2.2.2" data-path="regressão.html"><a href="regressão.html#regressão-com-máquinas-de-vetores-de-suporte"><i class="fa fa-check"></i><b>2.2.2</b> Regressão com máquinas de vetores de suporte</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classificação.html"><a href="classificação.html"><i class="fa fa-check"></i><b>3</b> Classificação</a><ul>
<li class="chapter" data-level="3.1" data-path="classificação.html"><a href="classificação.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.1</b> <em>k</em>-Nearest Neighbors</a><ul>
<li class="chapter" data-level="3.1.1" data-path="classificação.html"><a href="classificação.html#como-determinar-o-valor-de-k"><i class="fa fa-check"></i><b>3.1.1</b> Como determinar o valor de K ?</a></li>
<li class="chapter" data-level="3.1.2" data-path="classificação.html"><a href="classificação.html#complexidade"><i class="fa fa-check"></i><b>3.1.2</b> Complexidade</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classificação.html"><a href="classificação.html#árvore-de-decisão"><i class="fa fa-check"></i><b>3.2</b> Árvore de decisão</a><ul>
<li class="chapter" data-level="3.2.1" data-path="classificação.html"><a href="classificação.html#conceitos-gerais"><i class="fa fa-check"></i><b>3.2.1</b> Conceitos gerais</a></li>
<li class="chapter" data-level="3.2.2" data-path="classificação.html"><a href="classificação.html#funcionamento"><i class="fa fa-check"></i><b>3.2.2</b> Funcionamento</a></li>
<li class="chapter" data-level="3.2.3" data-path="classificação.html"><a href="classificação.html#problemas-com-overfitting"><i class="fa fa-check"></i><b>3.2.3</b> Problemas com overfitting</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Aprendizado Não Supervisionado</b></span></li>
<li class="chapter" data-level="4" data-path="agrupamento.html"><a href="agrupamento.html"><i class="fa fa-check"></i><b>4</b> Agrupamento</a><ul>
<li class="chapter" data-level="4.1" data-path="agrupamento.html"><a href="agrupamento.html#o-que-é-um-agrupamento"><i class="fa fa-check"></i><b>4.1</b> O que é um agrupamento?</a></li>
<li class="chapter" data-level="4.2" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-de-agrupamento"><i class="fa fa-check"></i><b>4.2</b> Técnicas de agrupamento</a><ul>
<li class="chapter" data-level="4.2.1" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-baseadas-em-partição"><i class="fa fa-check"></i><b>4.2.1</b> Técnicas baseadas em Partição</a></li>
<li class="chapter" data-level="4.2.2" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-baseadas-em-hierarquia"><i class="fa fa-check"></i><b>4.2.2</b> Técnicas baseadas em Hierarquia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="agrupamento.html"><a href="agrupamento.html#kmeans"><i class="fa fa-check"></i><b>4.3</b> Kmeans</a><ul>
<li class="chapter" data-level="4.3.1" data-path="agrupamento.html"><a href="agrupamento.html#como-avaliar-o-kmeans"><i class="fa fa-check"></i><b>4.3.1</b> Como avaliar o Kmeans?</a></li>
<li class="chapter" data-level="4.3.2" data-path="agrupamento.html"><a href="agrupamento.html#como-definir-a-quantidade-de-grupos"><i class="fa fa-check"></i><b>4.3.2</b> Como definir a quantidade de grupos?</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="agrupamento.html"><a href="agrupamento.html#agrupamento-hierárquico---método-aglomerativo"><i class="fa fa-check"></i><b>4.4</b> Agrupamento Hierárquico - Método Aglomerativo</a><ul>
<li class="chapter" data-level="4.4.1" data-path="agrupamento.html"><a href="agrupamento.html#qual-método-de-ligação-deve-ser-usado"><i class="fa fa-check"></i><b>4.4.1</b> Qual método de ligação deve ser usado?</a></li>
<li class="chapter" data-level="4.4.2" data-path="agrupamento.html"><a href="agrupamento.html#como-visualizar-os-grupos-no-dendrograma"><i class="fa fa-check"></i><b>4.4.2</b> Como visualizar os grupos no dendrograma?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="exemplos.html"><a href="exemplos.html"><i class="fa fa-check"></i><b>5</b> Exemplos</a></li>
<li class="chapter" data-level="6" data-path="considerações-finais.html"><a href="considerações-finais.html"><i class="fa fa-check"></i><b>6</b> Considerações finais</a></li>
<li class="part"><span><b>IV Apendice</b></span></li>
<li class="chapter" data-level="7" data-path="sobre-o-kaggle.html"><a href="sobre-o-kaggle.html"><i class="fa fa-check"></i><b>7</b> Sobre o Kaggle</a><ul>
<li class="chapter" data-level="7.1" data-path="sobre-o-kaggle.html"><a href="sobre-o-kaggle.html#cadastro"><i class="fa fa-check"></i><b>7.1</b> Cadastro</a></li>
<li class="chapter" data-level="7.2" data-path="sobre-o-kaggle.html"><a href="sobre-o-kaggle.html#criação-de-um-notebook"><i class="fa fa-check"></i><b>7.2</b> Criação de um notebook</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="referências-bibliográficas.html"><a href="referências-bibliográficas.html"><i class="fa fa-check"></i><b>8</b> Referências Bibliográficas</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introdução ao Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regressão" class="section level1">
<h1><span class="header-section-number">2</span> Regressão</h1>
<p>Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade
de prever algum acontecimento futuro. Estamos a todo momento assimilando
informações para realizar alguma tomada de decisão, seja de forma intrínseca ou
não. No contexto de Machine Learning (ML) isso é feito pelas técnicas de
regressão. A regressão é uma ferramenta que busca modelar relações entre
variáveis dependentes e independentes através de métodos estatísticos
<span class="citation">(Soto <a href="#ref-soto2013regression">2013</a>)</span>.</p>
<p>Uma variável independente, normalmente representada pela letra <span class="math inline">\(x\)</span>,
caracteriza uma grandeza que está sendo manipulada durante um experimento e que
não sofre influência de outras variáveis. Já a variável dependente, normalmente
representada pela letra <span class="math inline">\(y\)</span>, caracteriza valores que estão diretamente
associados à variável independente, ou seja, de forma direta ou indireta <span class="math inline">\(x\)</span>
excerce influência sobre <span class="math inline">\(y\)</span>. Na Figura <a href="regressão.html#fig:happinessWorld">2.1</a> é apresentada
a relação entre a expectativa de vida baseada e um índice de felicidade
calculado em diversos países obtidos a partir de um levantamento feito por
<span class="citation">Helliwell et al. (<a href="#ref-helliwell2020social">2020</a>)</span>. A variável independente nesse exemplo é representada pelo
índice de felicidade e a expectativa de vida age como variável independente,
dessa forma pode ser observada uma tendência de expectativa de vida maior em
países com alto índice de felicidade, com uma força de correlação de 0,77.</p>
<div class="figure" style="text-align: center"><span id="fig:happinessWorld"></span>
<img src="assets/03_regression/happiness_world.png" alt="Relação entre o índice de felicidade e expectativa de vida. Fonte: [@helliwell2020social]" width="1847" />
<p class="caption">
Figure 2.1: Relação entre o índice de felicidade e expectativa de vida. Fonte: <span class="citation">(Helliwell et al. <a href="#ref-helliwell2020social">2020</a>)</span>
</p>
</div>
<p>As relações entre as variáveis dependentes e independentes são feitas através de
algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas
é o coeficiente de Pearson, que mede a associação linear entre duas variáveis
<span class="citation">(Kirch <a href="#ref-kirck2008pearson">2008</a>)</span>. Esse coeficiente de correlação pode ser definido pela
Equação <a href="regressão.html#eq:corr-pearson">(2.1)</a>, onde <span class="math inline">\(n\)</span> é o total de amostras, <span class="math inline">\(\overline{x}\)</span>
e <span class="math inline">\(\overline{y}\)</span> são as médias aritméticas de ambas as variáveis. Os valores do
coeficiente de Pearson variam entre -1 e 1, de tal forma que quanto mais
próximos desses extremos, melhor correlacionado estão as variáveis. A Figura
<a href="regressão.html#fig:scatterCorrelations">2.2</a> mostra alguns exemplos com gráficos de dispersão
de variáveis com diferentes correlações.</p>
<p><span class="math display" id="eq:corr-pearson">\[\begin{equation} 
    r_{xy} = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
    {\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}
\tag{2.1}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:scatterCorrelations"></span>
<img src="assets/03_regression/correlations.png" alt="Diferentes correlações entre variáveis. Fonte: [@helliwell2020social]" width="2014" />
<p class="caption">
Figure 2.2: Diferentes correlações entre variáveis. Fonte: <span class="citation">(Helliwell et al. <a href="#ref-helliwell2020social">2020</a>)</span>
</p>
</div>
<p>Os métodos de regressão se utilizam dessas correlações entre as variáveis para
estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem
sempre essas correlações são tão explícitas assim, sendo necessário outras
abordagens mais robustas para realizar as previsões. Em ML os modelos de
regressão podem ser criados a partir de diversas abordagens, desde as mais
simples com poucas configurações de parâmetros e de fácil interpretação do
funcionamento, até as abordagens mais complexas. Os métodos de regressão
abordados neste capítulo serão <code>Regressão linear</code>,
<code>Máquina de vetores de suporte</code> e <code>Árvores de decisão</code>.</p>
<div id="regressão-linear" class="section level2">
<h2><span class="header-section-number">2.1</span> Regressão Linear</h2>
<p><a href="https://www.kaggle.com/lordadriano/mc2-worcap-2020-linear-regression"><img src="https://suspicious-wescoff-e06084.netlify.app/badge-perguntas.svg" alt="Questao disponivel" /></a></p>
<p>A regressão linear é um dos métodos mais intuitivos e utilizados para essa
finalidade. Esses métodos são divididos em dois grupos, a regressão linear simples
(RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma
relação entre duas variáveis através de uma função, que pode ser definida por:</p>
<p><span class="math display" id="eq:rls-function">\[\begin{equation} 
    y_{i} = \alpha+\beta x_{i}
\tag{2.2}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(y_{i}\)</span> é a variável alvo, <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta x_{i}\)</span> são coeficientes
calculados pela regressão, que representam o intercepto no eixo <span class="math inline">\(y\)</span> e inclinação
da reta, respectivamente.</p>
<p>A RLM é semelhante semelhante à RLS, porém possui múltiplas variáveis
preditoras, e pode ser definida por:</p>
<p><span class="math display" id="eq:rlm-function">\[\begin{equation} 
    y_{i} = \alpha+\beta x_{i1}+\beta x_{i2}+...+\beta x_{in}
\tag{2.3}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(y_{i}\)</span> é a variável alvo, <span class="math inline">\(\alpha\)</span> continua sendo o coeficiente de
intercepto e <span class="math inline">\(\beta x_{ip}\)</span> o é coeficiente angular da <span class="math inline">\(p\)</span>-ésima variável. Ambos
os métodos podem ainda serem somados a um termo <span class="math inline">\(\epsilon\)</span> de erro.</p>
<div id="coeficientes-da-regressão-linear" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Coeficientes da regressão linear</h3>
<p>Existem diversas abordagens para se calcular os coeficientes <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>
da equação da regressão linear, as técnicas baseadas em mínimos quadrados
ordinários e gradiente descendente são as mais comuns. A seguir serão
apresentados os funcionamentos dessas abordagens.</p>
<div id="métodos-dos-quadrados-ordinários" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Métodos dos quadrados ordinários</h4>
<p>O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ),
busca encontrar o melhor valor para os coeficientes citados anteriormente, de
tal forma que a diferença absoluta entre o valor real e o predito pela função
seja a menor possível entre todos os pontos. A Figura <a href="regressão.html#fig:ols">2.3</a> mostra um
exemplo de regressão linear utilizando o MQO para o conjunto de pontos descritos
na tabela a seguir:</p>
<table>
<thead>
<tr class="header">
<th align="center">Variável independente</th>
<th align="center">Variável dependente</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0,44</td>
<td align="center">5,52</td>
</tr>
<tr class="even">
<td align="center">1,74</td>
<td align="center">8,89</td>
</tr>
<tr class="odd">
<td align="center">0,41</td>
<td align="center">4,05</td>
</tr>
<tr class="even">
<td align="center">1,84</td>
<td align="center">9,31</td>
</tr>
<tr class="odd">
<td align="center">0,98</td>
<td align="center">6,57</td>
</tr>
<tr class="even">
<td align="center">1,22</td>
<td align="center">8,27</td>
</tr>
<tr class="odd">
<td align="center">1,53</td>
<td align="center">6,93</td>
</tr>
<tr class="even">
<td align="center">1,04</td>
<td align="center">6,41</td>
</tr>
<tr class="odd">
<td align="center">0,59</td>
<td align="center">6,93</td>
</tr>
<tr class="even">
<td align="center">0,38</td>
<td align="center">6,98</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:ols"></span>
<img src="assets/03_regression/ols.png" alt="Exemplo do método dos quadrados ordinários." width="1220" />
<p class="caption">
Figure 2.3: Exemplo do método dos quadrados ordinários.
</p>
</div>
<p>Para se chegar no resultado apresentado na Figura <a href="regressão.html#fig:ols">2.3</a>, os
coeficientes da regressão linear foram ajustados utilizando derivadas parciais,
de tal tal forma que o erro quadrático médio entre entre a função e cada um dos
pontos fossem a menor possível. A Figura <a href="regressão.html#fig:ols-steps">2.4</a> mostra o ajuste
dos coeficientes da equação em relação a cada ponto.</p>
<div class="figure" style="text-align: center"><span id="fig:ols-steps"></span>
<img src="assets/03_regression/ols-steps.png" alt="Ajuste da regressão linear por método dos quadrados ordinários." width="2390" />
<p class="caption">
Figure 2.4: Ajuste da regressão linear por método dos quadrados ordinários.
</p>
</div>
</div>
<div id="gradiente-descendente" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> Gradiente descendente</h4>
<p>O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para
otimização de modelos de ML. Este é um método interativo que busca encontrar os
coeficiente <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> através da minimização de uma função de custo,
que normalmente é o erro quadrático médio (MSE - sigla do inglês,
<em>mean squared error</em>).</p>
<p>O GD funciona de forma iterativa e inicializa os coeficientes com um valor
predefinido ou aleatório. Em cada iteração é obtido o somatório do erro entre
todos os valores das variáveis dependentes e valores calculados pela função. Com
base nesse erro e em uma taxa de aprendizagem do modelo pré definida, os valores
dos coeficientes da função são atualizados para a próxima iteração. A taxa de
aprendizagem deve ser definida com um valor equilibrado. A definição de um valor
muito alto para a taxa de aprendizagem pode levar o modelo a cair em um mínimo
local, ou seja, o modelo não consegue chegar em seu melhor ajuste. Já quando a
taxa de aprendizagem é definida com um valor muito baixo, o modelo demora mais
tempo para chegar no ajuste ideal, necessitando de muito mais tempo e
processamento até que haja a convergência. A Figura <a href="regressão.html#fig:learning-rate">2.5</a>
mostra o comportamento do GD com diferentes categorias de valores mencionadas
para a taxa de aprendizagem.</p>
<div class="figure" style="text-align: center"><span id="fig:learning-rate"></span>
<img src="assets/03_regression/learning-rate-gd.png" alt="Problemas na taxa de aprendizado do gradiente descendente." width="3410" />
<p class="caption">
Figure 2.5: Problemas na taxa de aprendizado do gradiente descendente.
</p>
</div>
<p>Os principais parâmetros a serem definidos nessa abordagem são a taxa de
aprendizagem e o número de iterações. Considerando os pontos utilizados no
exemplo anterior, foi aplicada a regressão linear utilizando o GD como método de
atualização dos coeficientes. A Figura <a href="regressão.html#fig:gd-example1">2.6</a> mostra o ajuste da
função, custo e os coeficientes <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> ao longo de 50 iterações com
taxa de aprendizado muito baixa. Na Figura <a href="regressão.html#fig:gd-example1">2.6</a> pode ser
observado que as iterações finalizam antes da convergência do modelo.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example1"></span>
<img src="assets/03_regression/gradient-descendent-small.png" alt="Regressão linear com taxa de aprendizagem baixa no gradiente descendente." width="2440" />
<p class="caption">
Figure 2.6: Regressão linear com taxa de aprendizagem baixa no gradiente descendente.
</p>
</div>
<p>Como mencionado anteriormente, uma taxa de aprendizagem muito grande também
interfere no ajuste dos coeficientes, uma vez o modelo não consegue atingir o
mínimo global. A Figura <a href="regressão.html#fig:gd-example2">2.7</a> mostra o resultado da execução da
regressão linear utilizando uma taxa de aprendizagem muito grande no GD.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example2"></span>
<img src="assets/03_regression/gradient-descendent-large.png" alt="Regressão linear com taxa de aprendizagem alta no gradiente descendente." width="2422" />
<p class="caption">
Figure 2.7: Regressão linear com taxa de aprendizagem alta no gradiente descendente.
</p>
</div>
<p>Já com uma taxa de aprendizagem equilibrada, o GD é capaz de ajustar os
coeficientes de forma mais eficiente. A Figura <a href="regressão.html#fig:gd-example3">2.8</a> mostra
o resultado do algoritmo executado com uma taxa de aprendizagem mais
equilibrada. Como os valores iniciais dos coeficientes são definidos de forma
aleatória, nas primeiras iterações o gradiente apresenta uma alta perturbação,
que vai se atenuando ao longo das épocas.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example3"></span>
<img src="assets/03_regression/gradient-descendent.png" alt="Regressão linear com taxa de aprendizagem equilibrada no gradiente descendente." width="2422" />
<p class="caption">
Figure 2.8: Regressão linear com taxa de aprendizagem equilibrada no gradiente descendente.
</p>
</div>
<p>Para dados com poucas dimensões, ou seja, poucas variáveis preditoras, o MQO é
mais recomendado, pois diferente do GD, não é um algoritmo interativo, e sua
complexidade está associada diretamente à quantidade de pontos. Já o GD tem
melhor performance quando os dados possuem muitas dimensões.</p>
<p>A regressão linear pode ser aplicada em uma vasta variedade de problemas, mas
como foi apresentado ao longo desta seção, é necessário que os dados possuam uma
alta correlação para ela funcionar perfeitamente bem. Este algoritmo está
implementado nas principais principais bibliotecas de ML em diferentes
linguagens de programação. Em Python a regressão linear está disponível na
biblioteca <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">Scikit-learn</a>.</p>
</div>
</div>
</div>
<div id="máquina-de-vetores-de-suporte" class="section level2">
<h2><span class="header-section-number">2.2</span> Máquina de vetores de suporte</h2>
<p><a href="https://www.kaggle.com/lordadriano/mc2-worcap-2020-svr"><img src="https://suspicious-wescoff-e06084.netlify.app/badge-perguntas.svg" alt="Questao disponivel" /></a></p>
<p>A máquina de vetores de suporte (SVM - sigla do inglês,
<em>support vector machine</em>) é um modelo de aprendizado de máquina supervisionado
concebido a partir de um conceito inicialmente proposto por <span class="citation">Vapnik and Chervonenkis (<a href="#ref-vapnik1965class">1963</a>)</span>.
A SVM podem ser utilizada tanto para tarefas de classificação, quanto para
tarefas de regressão, sendo uma ótima alternativa aos modelos de redes neurais
artificiais profundas que tem custo computacional muito superior em dados
com muitas dimensões. Outra vantagem na utilização dos modelos baseados em SVM é
que eles não são sensíveis aos <em>outliers</em>, ou seja, valores extremos não causam
ruído no treinamento.</p>
<p>O funcionamento básico das SVM consiste em ajustar a equação de uma reta,
denominada hiperplano de tal forma que a distância entre ela e os pontos com
características diferentes seja maximizada. Um conjunto de <span class="math inline">\(n\)</span> pontos é definido
como <span class="math inline">\((\vec{x_{1}}, y_{1}), (\vec{x_{2}}, y_{2}), ..., (\vec{x_{n}}, y_{n})\)</span>,
onde <span class="math inline">\(\vec{x_{i}}\)</span> são as variáveis independentes representadas por um vetor de
<span class="math inline">\(d\)</span>-dimensões e <span class="math inline">\(y_{i}\)</span> são as variáveis dependentes. A distância maximizada
entre o hiperplano e as fronteiras são definidas como margens e os pontos que
estão no limite dessa margem são os vetores de suporte. Esses componentes podem
ser modelados da seguinte forma:</p>
<p><span class="math display" id="eq:svm-components">\[\begin{equation} 
    \vec{w}\cdot\vec{x}-b =
      \begin{cases}
        &amp; -1, &amp; \text{primeiro vetor de suporte} \\
        &amp; 0, &amp; \text{hiperplano} \\
        &amp; 1, &amp; \text{segundo vetor de suporte}
      \end{cases}
\tag{2.4}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(\vec{w}\)</span> é um vetor perpendicular aos pontos, <span class="math inline">\(\vec{x}\)</span> é o vetor do
conjunto de pontos é <span class="math inline">\(b\)</span> é uma constante opcional que pode ser usada como uma
<em>bias</em>. Quando o resultado dessa equação é igual a <span class="math inline">\(1\)</span> ou <span class="math inline">\(-1\)</span> trata-se de um
dos vetores de suporte, já com o resultado maior que <span class="math inline">\(0\)</span> e menor que
<span class="math inline">\(1\)</span> ou menor que <span class="math inline">\(0\)</span> e maior que <span class="math inline">\(-1\)</span> trata-se de um espaço da margem. A Figura
<a href="regressão.html#fig:linear-svm">2.9</a> mostra um exemplo da aplicação do algoritmo SVM em um
conjunto de dados linearmente separável. Nessa figura, o hiperplano é
caracterizado pela linha contínua, os vetores de suporte são as linhas
tracejadas que interceptam os pontos com contorno destacado, e o espaço entre
eles são as margens.</p>
<div class="figure" style="text-align: center"><span id="fig:linear-svm"></span>
<img src="assets/03_regression/linear-svm.png" alt="SVM para conjunto de dados linearmente separáveis." width="995" />
<p class="caption">
Figure 2.9: SVM para conjunto de dados linearmente separáveis.
</p>
</div>
<p>As primeiras versões das SVM eram limitadas somente para resolução de problemas
linearmente separáveis, como mostrado no exemplo anterior, mas a grande maioria
dos problemas não são linearmente separáveis. Considerando a Figura
<a href="regressão.html#fig:kernels-problem">2.10</a> é muito difícil traçar um hiperplano que separe bem
os pontos de cores diferentes. Uma alternativa para esse problema é aumentar as
dimensões para a representação do hiperplano. Essa tarefa é feita com a
introdução de um conceito definido <em>kernel</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels-problem"></span>
<img src="assets/03_regression/kernels-problem.png" alt="Conjunto de dados não linearmente separáveis." width="910" />
<p class="caption">
Figure 2.10: Conjunto de dados não linearmente separáveis.
</p>
</div>
<p>Ao traçar um hiperplano não linear com a utilização de <em>kernels</em> é possível
ajustar melhor os vetores de suporte aos dados. A Figura <a href="regressão.html#kernels">2.2.1</a> mostra o
conjunto de dados ajustado com hiperplanos lineares e não lineares.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels"></span>
<img src="assets/03_regression/kernels.png" alt="Hiperplanos utilizando *kernels* com funções lineares e não lineares." width="1925" />
<p class="caption">
Figure 2.11: Hiperplanos utilizando <em>kernels</em> com funções lineares e não lineares.
</p>
</div>
<p>A abordagem utilizando os <em>kernels</em> é uma das principais características desse
modelo de ML, pois faz com que o hiperplano seja ajustado em uma dimensão
superior, utilizando equações de polinômios de grau maior. A Figura
<a href="regressão.html#fig:kernels-plot">2.12</a> mostra graficamente como é realizada essa manipulação.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels-plot"></span>
<img src="assets/03_regression/kernels-plot.png" alt="Representação gráfica dos dados e da função não linear." width="1816" />
<p class="caption">
Figure 2.12: Representação gráfica dos dados e da função não linear.
</p>
</div>
<p>A utilização de <em>kernels</em> é uma das principais características do SVM e faz com
que os modelos baseados nessa abordagem, sejam tão robustos quanto outras
técnicas mais complexas.</p>
<div id="kernels" class="section level3">
<h3><span class="header-section-number">2.2.1</span> <em>Kernels</em></h3>
<p>A utilização dos <em>kernels</em> em SVM foram introduzidos por <span class="citation">Boser, Guyon, and Vapnik (<a href="#ref-boser1992training">1992</a>)</span>.
<em>Kernels</em> são responsáveis por criar uma transformação dos
dados a partir de uma função, que são responsáveis por maximizar as margens dos
vetores de suporte. A maioria das bibliotecas de ML, já possuem <em>kernels</em>
implementados e também permitem a integração de outras funções customizadas. A
lista a seguir apresenta brevemente alguns dos principais <em>kernels</em>
utilizados.</p>
<ul>
<li><strong>Linear: </strong> Como mencionado anteriormente, é eficiente somente para problemas
linearmente separáveis, uma vez que seu ajuste se da através da equação de uma
reta. O <em>kernel</em> linear é definido apenas pelo produto entre duas amostras
<span class="math inline">\(\vec{x_{i}}\)</span> e <span class="math inline">\(\vec{x_{j}}\)</span>:</li>
</ul>
<p><span class="math display" id="eq:linear-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = \vec{x_{i}} \cdot \vec{x_{j}}
\tag{2.5}
\end{equation}\]</span></p>
<ul>
<li><strong>Polinomial: </strong> Os <em>kernels</em> polinomiais popularmente utilizados em tarefas
de processamento de imagens, permitem adicionar curvas aos hiperplanos. Além das
amostras <span class="math inline">\(\vec{x_{i}}\)</span> e <span class="math inline">\(\vec{x_{j}}\)</span>, o <em>kernel</em> polinomial também recebe o
a variável <span class="math inline">\(d\)</span> que indica o seu grau, como definido pela equação:</li>
</ul>
<p><span class="math display" id="eq:polynomial-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = (\vec{x_{i}} \cdot \vec{x_{j}} + 1)^{d}
\tag{2.6}
\end{equation}\]</span></p>
<ul>
<li><strong>Função gaussiana de base radial: </strong> Os <em>kernels</em> RBF
(<em>radial basis function</em>), como também são chamados, são recomendados quando não
se tem um conhecimento prévio acerca dos dados. Esse <em>kernel</em> realiza uma
transformação dos pontos utilizando uma função gaussiana, definida por:</li>
</ul>
<p><span class="math display" id="eq:rbf-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = exp \left(-\frac{\lVert\vec{x_{i}} - \vec{x_{j}}\rVert^2}{2\sigma^2} \right)
\tag{2.7}
\end{equation}\]</span></p>
</div>
<div id="regressão-com-máquinas-de-vetores-de-suporte" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Regressão com máquinas de vetores de suporte</h3>
<p>Embora a SVM seja aplicada principalmente para tarefas de classificação, ou
seja, geração de variáveis categóricas, ela também pode ser utilizada para
tarefas de regressão, calculando valores reais a partir de uma mudança na sua
função objetivo <span class="citation">(Drucker et al. <a href="#ref-drucker1997support">1997</a>)</span>. Essas abordagens são chamadas de
regressão por vetores de suporte (SVR - sigla do inglês,
<em>support vector regression</em>) e foram propostas por <span class="citation">Drucker et al. (<a href="#ref-drucker1997support">1997</a>)</span>.
Diferente de modelos tradicionais como as técnicas de regressão apresentadas na
seção anterior, que utilizam derivadas parciais para os cálculos dos intervalos
de confiança, no SVR os valores são previstos através dos hiperplanos.</p>
<p>Este método utiliza para a regressão a abordagem de classificação apresentada
anteriormente, porém com uma pequena variação na função objetivo, que agora
busca comportar dentro das margens comportar a maior quantidade de pontos. As
margens (<span class="math inline">\(\epsilon\)</span>) representam os intervalos de confiança e os vetores de
suporte que as delimitam, representam os limites para os erros positivos (<span class="math inline">\(\xi\)</span>)
e negativos (<span class="math inline">\(\xi^{\ast}\)</span>). A Figura <a href="regressão.html#fig:svr">2.13</a> mostra uma representação de
um hiperplano não linear traçado para a regressão de dados.</p>
<div class="figure" style="text-align: center"><span id="fig:svr"></span>
<img src="assets/03_regression/svr.png" alt="Representação de hiperplano não linear para regressão. Adaptado de @drucker1997support." width="992" />
<p class="caption">
Figure 2.13: Representação de hiperplano não linear para regressão. Adaptado de <span class="citation">Drucker et al. (<a href="#ref-drucker1997support">1997</a>)</span>.
</p>
</div>

</div>
</div>
</div>
<h3> Referências Bibliográficas</h3>
<div id="refs" class="references">
<div id="ref-boser1992training">
<p>Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In <em>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</em>, 144–52.</p>
</div>
<div id="ref-drucker1997support">
<p>Drucker, Harris, Christopher JC Burges, Linda Kaufman, Alex J Smola, and Vladimir Vapnik. 1997. “Support Vector Regression Machines.” In <em>Advances in Neural Information Processing Systems</em>, 155–61.</p>
</div>
<div id="ref-helliwell2020social">
<p>Helliwell, John F, Haifang Huang, Shun Wang, and Max Norton. 2020. “Social Environments for World Happiness.” <em>World Happiness Report 2020</em>.</p>
</div>
<div id="ref-kirck2008pearson">
<p>Kirch, Wilhelm, ed. 2008. “Pearson’s Correlation Coefficient.” In <em>Encyclopedia of Public Health</em>, 1090–1. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-1-4020-5614-7_2569" class="uri">https://doi.org/10.1007/978-1-4020-5614-7_2569</a>.</p>
</div>
<div id="ref-soto2013regression">
<p>Soto, Timothy. 2013. “Regression Analysis.” In <em>Encyclopedia of Autism Spectrum Disorders</em>, edited by Fred R. Volkmar, 2538–8. New York, NY: Springer New York.</p>
</div>
<div id="ref-vapnik1965class">
<p>Vapnik, Vladimir N, and Alexey Y Chervonenkis. 1963. “On a Class of Pattern-Recognition Learning Algorithms.” <em>Automation and Remote Control</em> 25 (6). PLENUM PUBL CORP CONSULTANTS BUREAU, 233 SPRING ST, NEW YORK, NY 10013: 838.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introdução.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classificação.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
