[
["index.html", "Introdução ao Machine Learning Prefácio Grupo DataAt Nossos livros-textos Licença", " Introdução ao Machine Learning Adriano Almeida Felipe Carvalho Felipe Menino Prefácio Seja bem vinda(o) ao livro-texto do minicurso de Introdução ao Machine Learning. Criamos este material para compartilhar o pouco que sabemos e dividir nossas experiências. Neste material, você irá encontrar conteúdos sobre o conceito, técnicas e algumas dicas úteis sobre Machine Learning. Procuramos abordar os conceitos de forma didática, porque sabemos o quão difícil é se inteirar de uma nova área, principalmente para as pessoas que não estão familiarizadas com o assunto. Este livro-texto não tem um público-alvo, escrevemos com o objeto de atingir o máximo de pessoas em quaisquer áreas. O único pré-requisito para ler este livro-texto é ter curiosidade, porque não são as respostas que movem o mundo, e sim, as perguntas! Grupo DataAt O Dataat é um grupo de estudos composto por quatro integrantes: Adriano Almeida, Felipe Carvalho, Felipe Menino e Helvécio Neto, buscamos sempre aprimorar nosso conhecimento e compartilhá-lo. A ideia do grupo de estudo começou pelos Felipe’s, em 2017, quando estávamos na iniciação científica no Instituto Nacional de Pesquisas Espaciais (INPE). Até hoje não sabemos o porquê do nome Dataat, um dia, quando estivermos no auge do nosso conhecimento, talvez, saberemos explicar o que se passava na nossa cabeça. Nossos livros-textos No decorrer desses anos ministramos diversos mini-cursos, dentre eles: “Introdução à Análise de Dados” Livro-texto criado com o objetivo de mostrar os conceitos básicos de análise de dados com exemplos práticos com R e Python. “Introdução à Análise de Dados Espaciais” Livro-texto criado com objetivo de mostrar o uso de linguagens de programação na manipulação de dados espaciais. “Introdução ao Docker” feito pelo Felipe Menino, este livro almeja te ensinar os conceitos básicos do Docker. Licença Esta obra, como um todo, está licenciada sob uma Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["introdução.html", "1 Introdução 1.1 Machine learning", " 1 Introdução “As máquinas podem pensar?” A pergunta acima faz parte de um exercício teórico proposto pelo cientista da computação Alan Turing em seu artigo publicado em 1950 (TURING 1950). Conhecido também como jogo da imitação, o teste de Turing constitui, em sua concepção inicial, na interação entre três agentes: um agente interrogador e dois agentes respondentes, onde um dos agentes respondentes é um ser humano e outro uma máquina (computador). A pergunta enviada pelo agente interrogador é recebida por ambos os agentes respondentes, onde cada um deles devem enviar de volta a resposta. Com base nas respostas, o agente interrogador deve determinar quem é o humano é que é a máquina, a partir do momento em que esse agente não consegue mais fazer essa diferenciação, é dito que a máquina passou no teste. A Figura 1.1 mostra o esquema básico desse teste. Figure 1.1: Esquema do teste de Turing clássico. Diversas derivações deste teste surgiram posteriormente, o mais famoso deles e familiar entre a maioria dos internautas é o CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), mecanismo de segurança proposto por Von Ahn et al. (2003) para validar requisições através da resolução de pequenos desafios, que podem ser identificação de imagens ou textos distorcidos e com ruídos, e que tem como propósito dificultar o acesso não convencional a formulários, por exemplo, tentar impedir o uso bots. O teste de Turing talvez tenha sido um ponto de partida para o que hoje conhecemos por aprendizado de máquina (ML - sigla do inglês, Machine Learning) . A possibilidade de representar pensamentos em computadores, similares aos dos seres vivos foi um grande marco na humanidade. Atualmente esse conceito está sendo aplicado nas mais diversas áreas, tendo em algumas tarefas, o desempenho superior ao dos seres humanos. O próprio CAPTCHA é um exemplo disso, em algumas de suas versões iniciais o conteúdo ficava tão distorcido, que acabava dificultando a sua identificação pelos humanos, em contrapartida, os algoritmos conseguiam resolver o desafio com certa facilidade. Neste capítulo, será apresentada uma visão geral sobre o Machine Learning, discorrendo sobre as principais classes de algoritmos e aplicações com ênfase na área espacial. Ao final deste capítulo o leitor deverá ser capaz de: Compreender o contexto histórico e a definição do ML; Diferenciar as principais abordagens de treinamento dos modelos de ML; Diferenciar as principais classes de algoritmos de ML; Compreender as etapas mínimas necessárias para a produção de um modelo de ML; 1.1 Machine learning O aprendizado de máquina é uma das principais subáreas da inteligência artificial, e é composto por uma coleção de métodos criados a partir de modelos matemáticos baseados na teoria estatística que permitem aos computadores automatizar tarefas com base na descoberta sistemática de padrões nos conjuntos de dados disponíveis ou em experiências passadas (Bhavsar et al. 2017; Alpaydin 2020). Segundo a definição de Samuel (1959), um dos pioneiros do assunto, o aprendizado de máquina é “um campo de estudo que oferece aos computadores a capacidade de aprender sem serem explicitamente programados”. Segundo a definição de Mitchell (1997), é dito que um computador aprende com a experiência \\(E\\) a repeito de alguma classe de tarefas \\(T\\) e desempenho medido por \\(P\\), se seu desempenho nas tarefas em \\(T\\), conforme medido por \\(P\\), melhora com a expêriencia \\(E\\), confuso? Então vamos a um exemplo: Imagine que você está desenvolvendo um programa para prever o acumulado de precipitação na próxima hora a partir de dados anteriores. A tarefa \\(T\\) seria estimar o acumulado de precipitação na próxima hora, a medida de desempenho \\(P\\) poderia ser alguma métrica de erro, como a diferença entre o valor previsto e o observado, já a experiência \\(E\\) seria as várias tentativas de realizar a previsão. O programa aprende a medida que sua previsão se aproxima do valor observado durante suas experiências. A forma com que o programa aprende, está associada a um conjunto de configurações previamente definidas, denominadas de hiperparâmetros. Inicialmente, há uma certa subjetividade envolvida na definição inicial dos hiperparâmetros dos modelos, que ao longo do seu desenvolvimento vão sendo ajustados em conformidade com os dados. O processo de ajuste dos hiperparâmetros com o intuito de melhorar o desempenho do modelo é conhecido como fine-tuning. O conjunto de hiperparâmetros está associado ao tipo de modelo que está sendo desenvolvido, que por sua vez possuem características de aprendizado diferentes, conforme mostrado na Figura 1.2. Figure 1.2: Diagrama dos tipos de aprendizado em machine learning. 1.1.1 Aprendizado supervisionado No aprendizado supervisionado, o modelo recebe um conjunto de entradas com suas respectivas saídas e busca encontrar uma função que estabelaça uma relação aproximada entre elas. Mais formalmente, o modelo baseado no aprendizado supervisionado busca encontrar uma função \\(h(x_{i})\\), denominada hipótese, que se aproxime da função \\(f(x_{i})\\), onde \\(f(x_{i})\\) é a saída da \\(i\\)-ésima entrada de \\(x\\) (Russell and Norvig 2002). Os hiperparâmetros dos modelos baseados em aprendizado supervisionado são configurados com intuito de calibrar seu nível de assertividade e precisão. Essas características estão associadas ao bias e variância do modelo. O bias está relacionado à capacidade do modelo se ajustar aos dados aos quais lhes foram apresentados durante o treinamento. Já a variância é a variabilidade das previsões do modelo. A complexidade do modelo aumenta a medida que ele vai se ajustando aos dados, em contrapartidada vai perdendo também a sua capacidade de generalização, que faz com que variância seja aumentada. Os hiperparâmetros devem ser configurados de tal forma equilibrar o bias e a variância, este equilíbrio é denominado trade-off. Para modelos lineares, a complexidade do modelo deve ser ajustada de tal forma que o bias e a variância tenham o menor valor possível. Já para modelos não lineares, o ponto de equilibrio deve ser onde a complexidade do podelo possui o menor bias e maior variância. A Figura 1.3 mostra um esquema para a complexidade ideal em modelos lineares e não lineares. Figure 1.3: Esquema do trade-off no aprendizado supervisionado. O ajuste desbalanceado da complexidade do modelo pode acarretar nos problemas de underfitting (sub-ajuste) e o overfitting (superajuste). O problema de underfitting está associado a falta de capacidade do modelo na representação dos dados. Já no overfitting o modelo se ajusta muito aos dados e perde a sua capacidade de generalização, que faz com que o erro seja muito alto ao ser apresentado novas amostras. A Figura 1.4 mostra um exemplo com diferentes ajustes do modelo aos dados. Figure 1.4: Diferentes ajustes do modelo aos dados. Os modelos de aprendizado supervisionado estão associados às tarefas de regressão e classificação. Nas tarefas de regressão, o modelo deve buscar o ajuste de uma função que melhor aproxime os dados de entrada com os dados de saída. Já os modelos de classificação buscam o ajuste em uma função que mellhor separe um conjunto de variáveis categóricas. Os modelos de regressão e classificação são melhor apresentados neste livro nos capítulos 2 e 3 , respectivamente. 1.1.2 Aprendizado Não supervisionado O aprendizado não supervisionado, diferente do aprendizado supervisionado, deve fazer inferências a partir de um conjunto de dados que não foi rotulado, classificado ou categorizado previamente. Este tipo de aprendizado é amplamente utilizado para a descoberta de padrões ocultos nos dados. Este tipo de abordagem segue o fluxo apresentado na Figura 1.5. Figure 1.5: Fluxo de execução do aprendizado não supervisionado. As tarefas de agrupamento e redução de dimensionalidade estão entre as principais tarefas executadas pelos algoritmos de aprendizado não supervisionado. Essa abordagem também é amplamente utilizada para identificação de anomalias nos dados. Para as tarefas de agrupamento, o modelo recebe um conjunto de dados não rotulado, e partir disso busca agrupá-lo com base em alguma característica de similaridade, por exemplo, a distância entre os pontos. A quantidade de grupos pode ser definida previamente, ou pode ficar a cargo do próprio modelo. Os sistemas de recomendações, geralmente presentes na plataformas de entretenimento, é uma das principais aplicações que utilizam essa abordagem. No capítulo 4 deste livro as técnicas de agrupamento são apresentadas com mais detalhes. A redução de dimensionalidade é uma técnica que utiliza o aprendizado não supervisionado para a redução do número de variáveis. Essa técnica é utilizada para encontrar um número inferior de variáveis que melhor representam as características dos conjuntos de dados. Essa técnica é amplamente utilizada na detecção de bordas, no contexto de processamento digital de imagens. Por serem eventos raros, as anomalias podem ser difíceis de identificar, principalmente em uma grande quantidade de dados. O aprendizado não supervisionado pode ser utilizado na detecção dessas características. Uma das tarefas em que pode ser aplicado esse recurso é na detecção de transações fraudalentas. Além disso, a indentifiação de anomalias em um conjunto de dados, pode afetar no treinamento de um modelo utilizando o aprendizado supervisionado, agindo como ruído nos dados. 1.1.3 Aprendizado por reforço No aprendizado por reforço os modelos são treinados para tomarem uma sequência de decisões e um abiente incerto e complexo. Nessa abordagem, os agentes possuem um estado que é alterado após realizar uma ação que é executada de forma aleatória, com base nessa ação, os agentes podem ser penalizados ou recomempensados. Caso a ação do agente gere recompensas, então ela será reforçada para o seu próximo estado (Goodfellow et al. 2016). Nessa abordagem o modelo pode utilizar a tentativa e erro de forma a maximizar suas recompensas. A Figura 1.6 mostra o fluxo clássico da abordardagem baseada no aprendizado por reforço. Figure 1.6: Fluxo clássico do aprendizado por reforço. O ambiente é o local em que o agente pode interagir tomando suas decisões. A priori o agente não possui nenhuma informação a respeito do ambiente, mas ele vai o conhecendo no decorrer de suas experiências para evoluir seus estados. O estado diz respeito às condições atuais do agente e do ambiente. O estado do agente é atualizado com base em suas recompensas ou penalidades que são adiquiridas após suas ações. As ações são as interações do agente com o ambeiente. A recompensa é um sinal positivo que é ativado reforçando uma ação do agente, ja penalidade, é um sinal negativo que faz com que a ação do agente seja esquecida. Esse tipo de abordagem é aplamente utilizada em jogos. Com base em suas experiências, um agente agente pode aprender jogos com regras complexas como o xadrez. Nesse caso, o ambiente é o tabuleiro de xadrez, o estado é o posicionamento das peças, a ação é o movimento da peça, a recompensa é eliminar uma peça adversária e a penalidade é a perda de uma peça após o movimento. A aprendizado por reforço também está presente nos algoritmos dos veículos autônomos. Onde, o ambiente é o próprio local onde o veículo está presente, o estado é localização e percepção dos obstáculos capturadas pelos sensores, a ação são os comandos de direção, aceleração e freio, a recompensa é a a aproximação do destino e a penalidade pode ser a colisão com algum obstáculo. Referências Bibliográficas "],
["regressão.html", "2 Regressão 2.1 Regressão Linear 2.2 Máquina de vetores de suporte", " 2 Regressão Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade de prever algum acontecimento futuro. Estamos a todo momento assimilando informações para realizar alguma tomada de decisão, seja de forma intrínseca ou não. No contexto de Machine Learning (ML) isso é feito pelas técnicas de regressão. A regressão é uma ferramenta que busca modelar relações entre variáveis dependentes e independentes através de métodos estatísticos (Soto 2013). Uma variável independente, normalmente representada pela letra \\(x\\), caracteriza uma grandeza que está sendo manipulada durante um experimento e que não sofre influência de outras variáveis. Já a variável dependente, normalmente representada pela letra \\(y\\), caracteriza valores que estão diretamente associados à variável independente, ou seja, de forma direta ou indireta \\(x\\) excerce influência sobre \\(y\\). Na Figura 2.1 é apresentada a relação entre a expectativa de vida baseada e um índice de felicidade calculado em diversos países obtidos a partir de um levantamento feito por Helliwell et al. (2020). A variável independente nesse exemplo é representada pelo índice de felicidade e a expectativa de vida age como variável independente, dessa forma pode ser observada uma tendência de expectativa de vida maior em países com alto índice de felicidade, com uma força de correlação de 0,77. Figure 2.1: Relação entre o índice de felicidade e expectativa de vida. Fonte: (Helliwell et al. 2020) As relações entre as variáveis dependentes e independentes são feitas através de algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas é o coeficiente de Pearson, que mede a associação linear entre duas variáveis (Kirch 2008). Esse coeficiente de correlação pode ser definido pela Equação (2.1), onde \\(n\\) é o total de amostras, \\(\\overline{x}\\) e \\(\\overline{y}\\) são as médias aritméticas de ambas as variáveis. Os valores do coeficiente de Pearson variam entre -1 e 1, de tal forma que quanto mais próximos desses extremos, melhor correlacionado estão as variáveis. A Figura 2.2 mostra alguns exemplos com gráficos de dispersão de variáveis com diferentes correlações. \\[\\begin{equation} r_{xy} = \\frac{{}\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})} {\\sqrt{\\sum_{i=1}^{n} (x_i - \\overline{x})^2(y_i - \\overline{y})^2}} \\tag{2.1} \\end{equation}\\] Figure 2.2: Diferentes correlações entre variáveis. Fonte: (Helliwell et al. 2020) Os métodos de regressão se utilizam dessas correlações entre as variáveis para estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem sempre essas correlações são tão explícitas assim, sendo necessário outras abordagens mais robustas para realizar as previsões. Em ML os modelos de regressão podem ser criados a partir de diversas abordagens, desde as mais simples com poucas configurações de parâmetros e de fácil interpretação do funcionamento, até as abordagens mais complexas. Os métodos de regressão abordados neste capítulo serão Regressão linear, Máquina de vetores de suporte e Árvores de decisão. 2.1 Regressão Linear A regressão linear é um dos métodos mais intuitivos e utilizados para essa finalidade. Esses métodos são divididos em dois grupos, a regressão linear simples (RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma relação entre duas variáveis através de uma função, que pode ser definida por: \\[\\begin{equation} y_{i} = \\alpha+\\beta x_{i} \\tag{2.2} \\end{equation}\\] Onde \\(y_{i}\\) é a variável alvo, \\(\\alpha\\) e \\(\\beta x_{i}\\) são coeficientes calculados pela regressão, que representam o intercepto no eixo \\(y\\) e inclinação da reta, respectivamente. A RLM é semelhante semelhante à RLS, porém possui múltiplas variáveis preditoras, e pode ser definida por: \\[\\begin{equation} y_{i} = \\alpha+\\beta x_{i1}+\\beta x_{i2}+...+\\beta x_{in} \\tag{2.3} \\end{equation}\\] Onde \\(y_{i}\\) é a variável alvo, \\(\\alpha\\) continua sendo o coeficiente de intercepto e \\(\\beta x_{ip}\\) o é coeficiente angular da \\(p\\)-ésima variável. Ambos os métodos podem ainda serem somados a um termo \\(\\epsilon\\) de erro. 2.1.1 Coeficientes da regressão linear Existem diversas abordagens para se calcular os coeficientes \\(\\alpha\\) e \\(\\beta\\) da equação da regressão linear, as técnicas baseadas em mínimos quadrados ordinários e gradiente descendente são as mais comuns. A seguir serão apresentados os funcionamentos dessas abordagens. 2.1.1.1 Métodos dos quadrados ordinários O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ), busca encontrar o melhor valor para os coeficientes citados anteriormente, de tal forma que a diferença absoluta entre o valor real e o predito pela função seja a menor possível entre todos os pontos. A Figura 2.3 mostra um exemplo de regressão linear utilizando o MQO para o conjunto de pontos descritos na tabela a seguir: Variável independente Variável dependente 0,44 5,52 1,74 8,89 0,41 4,05 1,84 9,31 0,98 6,57 1,22 8,27 1,53 6,93 1,04 6,41 0,59 6,93 0,38 6,98 Figure 2.3: Exemplo do método dos quadrados ordinários. Para se chegar no resultado apresentado na Figura 2.3, os coeficientes da regressão linear foram ajustados utilizando derivadas parciais, de tal tal forma que o erro quadrático médio entre entre a função e cada um dos pontos fossem a menor possível. A Figura 2.4 mostra o ajuste dos coeficientes da equação em relação a cada ponto. Figure 2.4: Ajuste da regressão linear por método dos quadrados ordinários. 2.1.1.2 Gradiente descendente O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para otimização de modelos de ML. Este é um método interativo que busca encontrar os coeficiente \\(\\alpha\\) e \\(\\beta\\) através da minimização de uma função de custo, que normalmente é o erro quadrático médio (MSE - sigla do inglês, mean squared error). O GD funciona de forma iterativa e inicializa os coeficientes com um valor predefinido ou aleatório. Em cada iteração é obtido o somatório do erro entre todos os valores das variáveis dependentes e valores calculados pela função. Com base nesse erro e em uma taxa de aprendizagem do modelo pré definida, os valores dos coeficientes da função são atualizados para a próxima iteração. A taxa de aprendizagem deve ser definida com um valor equilibrado. A definição de um valor muito alto para a taxa de aprendizagem pode levar o modelo a cair em um mínimo local, ou seja, o modelo não consegue chegar em seu melhor ajuste. Já quando a taxa de aprendizagem é definida com um valor muito baixo, o modelo demora mais tempo para chegar no ajuste ideal, necessitando de muito mais tempo e processamento até que haja a convergência. A Figura 2.5 mostra o comportamento do GD com diferentes categorias de valores mencionadas para a taxa de aprendizagem. Figure 2.5: Problemas na taxa de aprendizado do gradiente descendente. Os principais parâmetros a serem definidos nessa abordagem são a taxa de aprendizagem e o número de iterações. Considerando os pontos utilizados no exemplo anterior, foi aplicada a regressão linear utilizando o GD como método de atualização dos coeficientes. A Figura 2.6 mostra o ajuste da função, custo e os coeficientes \\(\\alpha\\) e \\(\\beta\\) ao longo de 50 iterações com taxa de aprendizado muito baixa. Na Figura 2.6 pode ser observado que as iterações finalizam antes da convergência do modelo. Figure 2.6: Regressão linear com taxa de aprendizagem baixa no gradiente descendente. Como mencionado anteriormente, uma taxa de aprendizagem muito grande também interfere no ajuste dos coeficientes, uma vez o modelo não consegue atingir o mínimo global. A Figura 2.7 mostra o resultado da execução da regressão linear utilizando uma taxa de aprendizagem muito grande no GD. Figure 2.7: Regressão linear com taxa de aprendizagem alta no gradiente descendente. Já com uma taxa de aprendizagem equilibrada, o GD é capaz de ajustar os coeficientes de forma mais eficiente. A Figura 2.8 mostra o resultado do algoritmo executado com uma taxa de aprendizagem mais equilibrada. Como os valores iniciais dos coeficientes são definidos de forma aleatória, nas primeiras iterações o gradiente apresenta uma alta perturbação, que vai se atenuando ao longo das épocas. Figure 2.8: Regressão linear com taxa de aprendizagem equilibrada no gradiente descendente. Para dados com poucas dimensões, ou seja, poucas variáveis preditoras, o MQO é mais recomendado, pois diferente do GD, não é um algoritmo interativo, e sua complexidade está associada diretamente à quantidade de pontos. Já o GD tem melhor performance quando os dados possuem muitas dimensões. A regressão linear pode ser aplicada em uma vasta variedade de problemas, mas como foi apresentado ao longo desta seção, é necessário que os dados possuam uma alta correlação para ela funcionar perfeitamente bem. Este algoritmo está implementado nas principais principais bibliotecas de ML em diferentes linguagens de programação. Em Python a regressão linear está disponível na biblioteca Scikit-learn. 2.2 Máquina de vetores de suporte A máquina de vetores de suporte (SVM - sigla do inglês, support vector machine) é um modelo de aprendizado de máquina supervisionado concebido a partir de um conceito inicialmente proposto por Vapnik and Chervonenkis (1963). A SVM podem ser utilizada tanto para tarefas de classificação, quanto para tarefas de regressão, sendo uma ótima alternativa aos modelos de redes neurais artificiais profundas que tem custo computacional muito superior em dados com muitas dimensões. Outra vantagem na utilização dos modelos baseados em SVM é que eles não são sensíveis aos outliers, ou seja, valores extremos não causam ruído no treinamento. O funcionamento básico das SVM consiste em ajustar a equação de uma reta, denominada hiperplano de tal forma que a distância entre ela e os pontos com características diferentes seja maximizada. Um conjunto de \\(n\\) pontos é definido como \\((\\vec{x_{1}}, y_{1}), (\\vec{x_{2}}, y_{2}), ..., (\\vec{x_{n}}, y_{n})\\), onde \\(\\vec{x_{i}}\\) são as variáveis independentes representadas por um vetor de \\(d\\)-dimensões e \\(y_{i}\\) são as variáveis dependentes. A distância maximizada entre o hiperplano e as fronteiras são definidas como margens e os pontos que estão no limite dessa margem são os vetores de suporte. Esses componentes podem ser modelados da seguinte forma: \\[\\begin{equation} \\vec{w}\\cdot\\vec{x}-b = \\begin{cases} &amp; -1, &amp; \\text{primeiro vetor de suporte} \\\\ &amp; 0, &amp; \\text{hiperplano} \\\\ &amp; 1, &amp; \\text{segundo vetor de suporte} \\end{cases} \\tag{2.4} \\end{equation}\\] Onde \\(\\vec{w}\\) é um vetor perpendicular aos pontos, \\(\\vec{x}\\) é o vetor do conjunto de pontos é \\(b\\) é uma constante opcional que pode ser usada como uma bias. Quando o resultado dessa equação é igual a \\(1\\) ou \\(-1\\) trata-se de um dos vetores de suporte, já com o resultado maior que \\(0\\) e menor que \\(1\\) ou menor que \\(0\\) e maior que \\(-1\\) trata-se de um espaço da margem. A Figura 2.9 mostra um exemplo da aplicação do algoritmo SVM em um conjunto de dados linearmente separável. Nessa figura, o hiperplano é caracterizado pela linha contínua, os vetores de suporte são as linhas tracejadas que interceptam os pontos com contorno destacado, e o espaço entre eles são as margens. Figure 2.9: SVM para conjunto de dados linearmente separáveis. As primeiras versões das SVM eram limitadas somente para resolução de problemas linearmente separáveis, como mostrado no exemplo anterior, mas a grande maioria dos problemas não são linearmente separáveis. Considerando a Figura 2.10 é muito difícil traçar um hiperplano que separe bem os pontos de cores diferentes. Uma alternativa para esse problema é aumentar as dimensões para a representação do hiperplano. Essa tarefa é feita com a introdução de um conceito definido kernel. Figure 2.10: Conjunto de dados não linearmente separáveis. Ao traçar um hiperplano não linear com a utilização de kernels é possível ajustar melhor os vetores de suporte aos dados. A Figura 2.2.1 mostra o conjunto de dados ajustado com hiperplanos lineares e não lineares. Figure 2.11: Hiperplanos utilizando kernels com funções lineares e não lineares. A abordagem utilizando os kernels é uma das principais características desse modelo de ML, pois faz com que o hiperplano seja ajustado em uma dimensão superior, utilizando equações de polinômios de grau maior. A Figura 2.12 mostra graficamente como é realizada essa manipulação. Figure 2.12: Representação gráfica dos dados e da função não linear. A utilização de kernels é uma das principais características do SVM e faz com que os modelos baseados nessa abordagem, sejam tão robustos quanto outras técnicas mais complexas. 2.2.1 Kernels A utilização dos kernels em SVM foram introduzidos por Boser, Guyon, and Vapnik (1992). Kernels são responsáveis por criar uma transformação dos dados a partir de uma função, que são responsáveis por maximizar as margens dos vetores de suporte. A maioria das bibliotecas de ML, já possuem kernels implementados e também permitem a integração de outras funções customizadas. A lista a seguir apresenta brevemente alguns dos principais kernels utilizados. Linear: Como mencionado anteriormente, é eficiente somente para problemas linearmente separáveis, uma vez que seu ajuste se da através da equação de uma reta. O kernel linear é definido apenas pelo produto entre duas amostras \\(\\vec{x_{i}}\\) e \\(\\vec{x_{j}}\\): \\[\\begin{equation} k(\\vec{x_{i}}, \\vec{x_{j}}) = \\vec{x_{i}} \\cdot \\vec{x_{j}} \\tag{2.5} \\end{equation}\\] Polinomial: Os kernels polinomiais popularmente utilizados em tarefas de processamento de imagens, permitem adicionar curvas aos hiperplanos. Além das amostras \\(\\vec{x_{i}}\\) e \\(\\vec{x_{j}}\\), o kernel polinomial também recebe o a variável \\(d\\) que indica o seu grau, como definido pela equação: \\[\\begin{equation} k(\\vec{x_{i}}, \\vec{x_{j}}) = (\\vec{x_{i}} \\cdot \\vec{x_{j}} + 1)^{d} \\tag{2.6} \\end{equation}\\] Função gaussiana de base radial: Os kernels RBF (radial basis function), como também são chamados, são recomendados quando não se tem um conhecimento prévio acerca dos dados. Esse kernel realiza uma transformação dos pontos utilizando uma função gaussiana, definida por: \\[\\begin{equation} k(\\vec{x_{i}}, \\vec{x_{j}}) = exp \\left(-\\frac{\\lVert\\vec{x_{i}} - \\vec{x_{j}}\\rVert^2}{2\\sigma^2} \\right) \\tag{2.7} \\end{equation}\\] 2.2.2 Regressão com máquinas de vetores de suporte Embora a SVM seja aplicada principalmente para tarefas de classificação, ou seja, geração de variáveis categóricas, ela também pode ser utilizada para tarefas de regressão, calculando valores reais a partir de uma mudança na sua função objetivo (Drucker et al. 1997). Essas abordagens são chamadas de regressão por vetores de suporte (SVR - sigla do inglês, support vector regression) e foram propostas por Drucker et al. (1997). Diferente de modelos tradicionais como as técnicas de regressão apresentadas na seção anterior, que utilizam derivadas parciais para os cálculos dos intervalos de confiança, no SVR os valores são previstos através dos hiperplanos. Este método utiliza para a regressão a abordagem de classificação apresentada anteriormente, porém com uma pequena variação na função objetivo, que agora busca comportar dentro das margens comportar a maior quantidade de pontos. As margens (\\(\\epsilon\\)) representam os intervalos de confiança e os vetores de suporte que as delimitam, representam os limites para os erros positivos (\\(\\xi\\)) e negativos (\\(\\xi^{\\ast}\\)). A Figura 2.13 mostra uma representação de um hiperplano não linear traçado para a regressão de dados. Figure 2.13: Representação de hiperplano não linear para regressão. Adaptado de Drucker et al. (1997). Referências Bibliográficas "],
["classificação.html", "3 Classificação 3.1 k-Nearest Neighbors 3.2 Árvore de decisão", " 3 Classificação “Nossas características nos fazem igualmente diferentes” - Autor desconhecido É muito provável que você já tenha recebido algum tipo de SPAM em sua caixa de e-mail. Eles estão por toda parte, se disfarçando com os mais variados temas e assunto. Sua presença atrapalha se não esvaziarmos nossa caixa de email com periodicidade, em pouco tempo estamos com a caixa de entrada lotada! E não é só isso, como apresentado por Blanzieri and Bryl (2008), os SPAMs representam cerca de 75-80% de todos os e-mails que circulam na internet. Isso porque ainda nem entramos na extensa lista de problemas que os SPAMs causam diariamente. Como acha que podemos resolver este problema ? Uma possível solução pode ser a criação de algum tipo de mecanismo que nos ajude a identificar os SPAMs, para que possamos eliminá-los. Fazendo isso, provavelmente vamos evitar todo tipo de problema que esses “simples” e-mails podem causar. Aí vem mais uma questão, como podemos fazer a criação desse tal mecanismo ? Uma abordagem possível é criar um código que busque e identifique certos padrões nos e-mails e então indica se é ou não SPAM. Para um pensamento inicial parece ok, mas podem existir muitos padrões e que esse nosso código pode acabar ficando gigante ? E ainda mesmo com muitas regras, ele pode acabar não identificando alguma característica. Complicado né ? Felizmente para tarefas em que o problema é a identificação de padrões os algoritmos de Aprendizado de Máquina (Do inglês, Machine Learning - ML) podem nos ajudar! Através do uso de algoritmos de Classificação podemos resolver este problema! As técnicas de classificação são utilizadas para a identificação do rótulo de determinadas observações com base em características e informações previamente conhecidas (Lantz 2013) Para o uso dos algoritmos que realizam as atividades de Classificação, por eles pertencerem ao grupo de algoritmos supervisionados, o que precisamos é criar um conjunto de e-mails SPAM e então treinar o algoritmo para aprender as características mais relevantes dos SPAMs. Uma vez treinado, o algoritmo pode ser utilizado para identificar e-mails nunca antes vistos, já que ele mapeou as principais características, diferente da nossa ideia inicial de código, em que estávamos trabalhando com características específicas. Figure 3.1: Classificação de e-mails com ML - Fonte: Produção do autor Esta seção busca apresentar alguns dos principais algoritmos de ML supervisionado que podem ser utilizados para a realização das atividades de classificação. Serão apresentados os algoritmos k-Nearest Neighbors e Árvore de Decisão. 3.1 k-Nearest Neighbors Para começar, o primeiro algoritmo que vamos tratar será o k-Nearest Neighbors (kNN), que através da análise de vizinhança de amostras de um determinado conjunto de treinamento, define o rótulo das amostras do conjunto de teste. De maneira geral, o que o algoritmo faz é buscar os elementos que estão próximos à amostra que está sendo classificada, e com base nessas amostras que estão próximas faz a classificação. Então, aqui temos uma mistura de conceitos, vamos começar primeiro resumindo o algoritmo em uma frase: Me diga com quem tu andas, que eu digo quem tu és Essa frase ajuda muito a assimilar a ideia geral do algoritmo, mantenha ela em mente durante essa seção. Agora, precisamos entender que o kNN é dividido em duas partes principais: (i) Análise de vizinhança; (ii) Determinação do rótulo da classe. Vamos começar pela primeira parte. Análise de vizinhança; Nesta parte do algoritmo o que se busca é determinar quem são as amostras do conjunto de treinamento que estão mais próximas da amostra de teste que está sendo classificada. Mas nesse caso, o que é “estar próximo” ? De maneira intuitiva, a ideia de proximidade está relacionada a distância, e é exatamente dessa forma que o algoritmo, faz a determinação dos elementos próximos, para isso, utiliza várias funções de distância. Um exemplo de função de distância é a Distância Euclidiana, sim! a mesma que utilizamos para várias partes de nossas vidas para manipulação de elementos no espaço euclidiano. Você lembra que usamos ela para calcular a distância entre dois pontos ? Vamos olhar para relembrar Então, a Distância euclidiana é apresentada da seguinte forma \\[\\begin{equation} \\sqrt{\\sum_{i = 1}^{n} (p_i - q_i)^2} \\tag{3.1} \\end{equation}\\] Em que: \\(p_i =\\) Posição de uma determinada dimensão dos pontos \\(q_i =\\) Posição de outra dimensão que não a tratada em \\(p_i\\) Assim, através dessa função o algoritmo determina os elementos que estão mais próximos de uma determinada amostra. Cabe lembrar que, essa não é a única função, várias outras podem ser utilizadas. Agora, podemos ir para a segunda parte, vamos lá! Determinação do rótulo da classe. Para entender esta segunda parte, devemos nos lembrar que, esse algoritmo é um algoritmo supervisionado, então, quando estamos falando do conjunto amostral de treino, estamos assumindo que esses dados já possuem um rótulo definido e que esses rótulos serão utilizados no processo de classificação dos dados que não tem rótulo. Feito esse lembrete, vamos continuar! Nesta parte do algoritmo, após calcular a distância de cada um dos pontos, ele escolhe os k elementos mais próximos. Com a determinação desses elementos mais próximos, o algoritmo analisa e contabiliza, por classe, quantos são os elementos que compõem a vizinhança, dessa forma, ao final desse processo, o algoritmo sabe quais são as classes vizinhas da amostra a ser classificada e quantos elementos de cada uma dessas classes estão presentes na vizinhança. Feito isso, o algoritmo vai determinar o rótulo da amostra a ser classificada como sendo igual a classe, que na vizinhança possui a maior quantidade de elementos. Somente isso! Viu que interessante, esse é um algoritmo simples e que pode ser muito eficiente dependendo do contexto de uso. Mas espera aí, você pode estar se perguntando, “E a ideia do treino e teste que foi falada antes?”, bem, esse é um algoritmo de classificação lazzy learning, ou seja, a parte de treinamento é resumida em apenas organizar e armazenar os dados, de modo a deixar eles pronto para o uso. Agora, com o objetivo de deixar tudo mais claro, vamos para um exemplo visual, passo a passo da execução do algoritmo! Então, vamos começar considerando que o nosso conjunto amostral é formado por um grupo de pontos de várias cores, em que as cores representam os rótulos de cada um desses pontos e então, a ideia vai ser aplicar o kNN para determinar o rótulo de um novo ponto com base nesse conjunto já existente. Veja, aqui que os pontos que já existem representam o conjunto de treino, e o ponto que será classificado, representa o conjunto de teste Os pontos de treinamento são apresentados na Figura 3.2. Figure 3.2: Espaço euclidiano 1 - Fonte: Produção do autor Beleza! Então, com esse conjunto de dados, vamos adicionar um novo ponto, este é representado pelo ponto vermelho na figura abaixo. Perceba que, este é uma amostra que representa o conjunto de teste, e que, ao ser inserido neste espaço, ele não possui nenhum rótulo definido. Figure 3.3: Espaço euclidiano 2 - Fonte: Produção do autor Certo! Então já temos nosso problema de classificação definido, vamos resolver ele com o kNN. Aqui, o valor de k = 5 (Não se preocupe, vamos voltar nesse valor de k depois), ou seja, vamos buscar os 5 elementos mais próximos ao ponto vermelho. Feito essas considerações, vamos de maneira manual fazer a aplicação do algoritmo, começando com o primeiro passo, de análise de vizinhança, feito com o cálculo das distâncias. Aqui, você vai perceber que a distância utilizada foi a euclidiana, que apresentamos anteriormente. Figure 3.4: Cálculo de distância - Fonte: Produção do autor Com as distâncias calculadas a segunda parte do algoritmo, de determinação do rótulo com base nos vizinhos pode ser iniciada. Para isso, primeiro faz-se a seleção dos 5 elementos mais próximos. Figure 3.5: Seleção dos vizinhos mais próximos - Fonte: Produção do autor Com os vizinhos mais próximos determinados, é feito a contagem desses, separando cada um desses por rótulo, de modo que, a vizinhança é sumarizada em quantidade de elementos por rótulo, como apresentado na Figura 3.6. Figure 3.6: Contagem dos vizinhos mais próximos - Fonte: Produção do autor Então, é feita a determinação do rótulo do ponto que está sendo classificado, que como podemos ver, vai receber o rótulo laranja, uma vez que, esta é a classe que mais aparece na vizinhança do ponto vermelho. Figure 3.7: Definição da classe - Fonte: Produção do autor É desta forma que os passos que vimos do algoritmo kNN são materializados frente a um conjunto de dados. Viu, que legal! O que o algoritmo faz é exatamente aquilo que está na frase que apresentamos antes. 3.1.1 Como determinar o valor de K ? Legal! Então o algoritmo é simples de ser entendido, tem uma quantidade pequena de passos, todos compreensíveis. Mas uma pergunta pode ter surgido durante a explicação do algoritmo: Como determinar o valor de \\(K\\) ? Bem, essa é uma boa pergunta e mais que isso, é uma pergunta importante! Fazer a escolha da quantidade vizinhos que o kNN vai utilizar, determina o quão bem o algoritmo vai generalizar aos dados futuros. A escolha pelo valor de \\(K\\) deve ser feita considerando um equilíbrio entre um valor grande ou pequeno, isso porque cada um deles tem influências diretas na maneira a qual o algoritmo vai reconhecer os dados (Lantz 2013). Segundo Lantz (2013), quando valores muito grandes de \\(K\\) são escolhidos, tem-se uma redução do impacto causado por dados ruidosos, mas, pode fazer com que o algoritmo deixe de considerar características sucintas que estão nos dados, mas que são importantes para a correta determinação dos rótulos. Por outro lado, valores de \\(K\\) menores permitem o entendimento de características mais sofisticadas dos dados, mas sofrem muita influência de dados ruidosos. A Figura 3.8 faz uma representação da maneira como o processo de decisão para a determinação da classe pode ser tomado, neste, é possível perceber que os valores de \\(K\\) . Figure 3.8: Definição da classe - Fonte: Adaptado de Lantz (2013) Desta forma, a determinação do valor de \\(K\\) deve ser feita considerando os dados e as características que precisam ser mapeadas desses. 3.1.2 Complexidade Como você pode ter pensado, esse pode não ser um algoritmo computacionalmente muito barato, já que, o cálculo de distância nesse algoritmo que apresentamos, é sempre calculado entre todos os pontos do conjunto de treinamento com o conjunto de teste, o que se pensarmos em apenas poucas quantidades de pontos pode não ser um problema, mas que, com a crescente na quantidade, pode ser inviável. Para isso, considere a Figura 3.9, imagina que o ponto vermelho vai ser classificado, a distância dele para todos os outros pontos terá de ser determinada. Figure 3.9: Níveis de complexidade - Fonte: Produção do autor Como forma de reduzir essa complexidade e a quantidade de elementos que precisam ser contabilizados no cálculo da distância, várias implementações aplicam passos de indexação, com estruturas de dados como a KD-Tree e Quadtree, como é o caso do scikit-learn. Essa abordagem de implementação evita com que todos os pontos tenham de ser calculados, possibilitando assim que apenas os pontos necessários sejam contabilizados nos cálculos de distância. 3.2 Árvore de decisão A árvore de decisão é um dos algoritmos mais utilizados na área de ML, isso por apresentar bons resultados em diversos contextos e por ser considerado um método transparente, por deixar explícito as regras que estão sendo utilizadas para a tomada das decisões e geração dos resultados. No capítulo anterior, esta classe de algoritmos foi apresentada para a solução dos problemas de regressão, aqui, eles serão postos no contexto de classificação. 3.2.1 Conceitos gerais As árvores de decisão são fundamentalmente formas de representação de conhecimento através de uma estrutura hierárquica de perguntas na forma if-then-else. Isso faz com que a estrutura das árvores de decisão sejam semelhantes a um fluxograma, em que existem nós que são utilizados para representar as perguntas e desses são derivados outros nós, que podem representar a resposta ou mesmo outra pergunta. Essa estrutura é apresentada na Figura 3.10. Figure 3.10: Árvore de decisão - Fonte: Produção do autor Para nos familiarizarmos com os conceitos que estão envolvidos nessa representação, vamos olhar com calma cada um dos detalhes presentes na figura. Primeiro, a leitura desse tipo de árvore é feita sempre de cima para baixo já que a raiz da árvore está sempre no topo, como é o caso da Pergunta 1 neste exemplo. Além disso, note também que nessa estrutura existem dois tipos de elementos, as Perguntas e as Respostas, em que, das Perguntas podem sair outras dessas ou mesmo Respostas. As Respostas representam elementos finais e quando aparecem nada pode ser inserido abaixo. Outra coisa importante de ser citada é a característica recursiva das árvores de decisão, nessas, para cada ramo que é seguido após uma pergunta há uma nova árvore, que é criada através das mesmas regras de definição aplicadas na árvore anterior. Antes da aplicação em larga escala dos algoritmos de AM, essas estruturas já eram utilizadas, porém, com a diferença de que toda a sua criação era feita manualmente, através da aplicação do conhecimento vindo de pesquisas e experimentos empíricos. Por exemplo, um banco, antes do ML, ao querer ser mais assertivo nos empréstimos e diminuir a inadimplência, poderia utilizar de seu histórico de empréstimos e criar regras consultáveis que poderiam ser utilizadas como auxílio aos operadores que realizam empréstimos. A representação das regras definidas poderia ser feita através de uma árvore de decisão sem problemas, mas, isso não caracterizada nada de ML, já que todo o ‘processo de aprendizado’ foi feito manualmente por pessoas. Ao contrário disto, as árvores de decisão no contexto de ML são as responsáveis em olhar para os dados e decidir quais são as perguntas mais adequadas para uma determinada resposta. Isso é feito no algoritmo através de sucessivas divisões no conjunto de dados, de modo que, em cada divisão tem-se novos elementos adicionados na árvore. Para essa ideia ficar clara, vamos começar com um exemplo, neste, há um conjunto de pontos, em, cada cor representa uma classe. A árvore de decisão será treinada com esses dados de modo que novas classificações com base neste treinamento possam ser realizadas. O conjunto de dados e a representação da árvore são feitos na Figura 3.11. Figure 3.11: Conjunto de dados 1 - Fonte: Produção do autor Com o conjunto de dados definido, o primeiro passo realizado pela árvore é avaliar quais são as características que melhor definem uma determinada classe. Após fazer isso, a árvore cria uma pergunta que faz com que essa característica identificada como a melhor possa ser utilizada para a divisão do conjunto de dados. Neste caso, a árvore identificou que o conjunto de pontos da classe Vermelha estão majoritariamente nas posições com X acima de 10, então, é criada uma pergunta na árvore que verifica quais elementos são maiores que dez, ao fazer isso a divisão do conjunto de dados é realizada e então novos nós são adicionados na árvore, veja na Figura 3.12. Figure 3.12: Conjunto de dados 2 - Fonte: Produção do autor Se lembrarmos a definição feita anteriormente, temos que as árvores de decisão são estruturas recursivas, então, a mesma lógica de busca do elemento que melhor descreve um conjunto de dados e então a divisão é aplicado nos nós resultantes, isso é feito até que não haja mais elementos suficientes para a divisão ou quando em um nó todos os elementos pertencem a apenas uma classe. Na Figura 3.13 uma divisão é feita no nó esquerdo, gerado anteriormente, e no nó esquerdo não é feito mais nada. Figure 3.13: Conjunto de dados 3 - Fonte: Produção do autor Vamos supor agora que a árvore apresentada na Figura 3.13 finalizou seu treinamento, de modo que classificações podem ser iniciadas. Você pode se perguntar, “mas em alguns nós da árvore há uma mistura de elementos, como ele vai fazer a identificação da classe ?” Bem, este é um problema recorrente na aplicação das árvores de decisão, nem sempre as regras escolhidas vão criar grupos de dados 100% puros (de uma única classe). Mais regras poderia ser adicionadas, mas aí, caímos no problema de overfitting, em que a árvore começa a criar regras específicas para os dados que estão sendo utilizados no treinamento, o que faz seu desempenho ser muito ruim com dados que não sejam do conjunto utilizado no treino. Com relação a definição da classe, é feito a definição da classe aos nós. é definir a classe do nó gerado, considerando a classe mais representativa de cada nó, então, se formos classificar um novo ponto e ele tem X &gt; 10, ele vai ser colocado como classe Vermelha no conjunto de dados. Agora que temos uma visão geral do algoritmo, vamos dar ir adiante a frente e ver como os passos apresentados acima são feitos no algoritmo. 3.2.2 Funcionamento Nesta seção, vamos materializar as ideias que foram apresentadas na seção de Visão Geral, descrevendo os passos envolvidos em cada uma das etapas do treinamento da árvore de decisão que foi apresentado anteriormente. Se lembrarmos dos passos que vimos anteriormente, temos que a criação da árvore ocorre em três etapas Seleção do melhor atributo para a divisão dos registros Utilização deste atributo para a divisão do conjunto de dados, gerando novos nós na árvore Para cada nó gerado no passo 2, aplica-se recursivamente, o algoritmo de construção, iniciando no passo 1. O algoritmo deve parar a recursão e não dividir mais os nós quando: Todos os dados do nó pertencerem a uma mesma classe; Não há dados suficientes para a divisão; ou Todos os atributos disponíveis nos dados já foram utilizados. Agora, para que seja possível entender os conceitos utilizados na construção da árvore de decisão, vamos passar por cada um dos tópicos listados anteriormente. 3.2.2.1 Seleção do melhor atributo e divisão dos dados Como foi apresentado na visão geral, a primeira etapa do algoritmo de criação de uma árvore de decisão é a seleção do atributo que melhor divide o conjunto de dados. Naquele caso, o algoritmo decidiu fazer o uso do atributo X como base para a divisão, considerando também que o valor deste deveria ser menor que 10. Ambas essas decisões foram tomadas seguindo alguns critérios, que ajudaram a definir as perguntas que melhor dividem os dados. Esses critérios são apresentados nessa seção. Vamos começar com a seleção de atributos. Existem diversas métricas que podem auxiliar o algoritmo de criação da árvore de decisão a escolher os melhores atributos, sendo alguns deles o Índice GINI e o Ganho de informação. Neste documento, vamos considerar o uso da métrica de ganho de informação, neste, as divisões no conjunto de dados são feitas considerando a diminuição da entropia que esta vai causar no conjunto de dados. Calma, eu sei, é muito coisa de uma vez só, mas vamos por partes. Primeiro, a entropia é um conceito utilizado para determinar o grau de desordem do conjunto de dados. Antes de apresentar a fórmula, vamos dar uma olhada em diferentes conjuntos de dados e seus níveis de entropia. Figure 3.14: Níveis de entropia - Fonte: Produção do autor Perceba que, nos conjuntos de dados apresentados na Figura 3.14, quando menos mistura (Entenda como confusão neste caso) eu tenho no conjunto de dados, menor é a entropia do sistema. Então, a ideia base do ganho de informação é ir fazendo divisões de modo que os espaços resultantes da divisão tenham a menor entropia possível. Agora que já sabemos o que é, vejamos a fórmula da entropia: \\[\\begin{equation} E\\:=-\\:\\sum _{i=1}^{^N}\\left(p_i\\cdot log_2\\cdot p_i\\right)\\: \\tag{3.2} \\end{equation}\\] Onde, \\(N\\) = Quantidade total de elementos no conjunto de dados, \\(p_i\\) = Probabilidade de aparição da classe \\(i\\) no conjunto de dados Usando dessas duas informações a entropia do sistema pode ser gerada! Bem, e o Ganho de informação ? Então, ele representa a vantagem que um determinado atributo tem de ser utilizado para a divisão, onde essa vantagem representa o grau de diminuição da entropia do conjunto de dados. O ganho de informação é definido como: \\[\\begin{equation} GI\\left(Q\\right)=E_0-\\sum _{i=1}^q\\left(\\frac{N_i}{N}E_i\\right)\\: \\tag{3.3} \\end{equation}\\] Onde, \\(E_0\\) = Entropia do nó pai (Nó atual) \\(E_i\\) = Entropia do filho a ser gerado \\(q\\) = Quantidade de nós filho \\(N\\) = Quantidade total de elementos no conjunto de dados \\(N_i\\) = Quantidade de dados a ser inserido no nó filho \\(i\\) Uma vez tendo esses conceitos definidos, cabe deixar claro que, o ganho de informação será o elemento base utilizado no processo de seleção do atributo que melhor divide o conjunto de dados (que tem maior ganho de informação), e uma vez que este atributo é selecionado, ele passa a ser utilizado para a realização da divisão dos dados. Neste processo de divisão, mais passos podem ser considerados, mas o objetivo deste documento é de apenas apresentar a intuição geral por trás da árvore de decisão e seu processo de treinamento. 3.2.2.2 Exemplo de construção da árvore Para fechar essa parte mais teórica do funcionamento da árvore de decisão e podermos iniciar a parte prática, vamos fazer um exemplo passo a passo da criação de uma árvore de decisão. Neste exemplo, vamos utilizar os dados que estão apresentados na tabela abaixo, esses que foram postos com valores categóricos de maneira a tornar mais simples todo o cálculo passo a passo que será realizado. Está chovendo ? Está com tempo ? Transporte (Classe) Sim Sim Ônibus Sim Não Ônibus Não Sim Caminhada Não Não Ônibus Bom, nesta primeira etapa nossa árvore nem existe, é preciso definir para ela um ponto de partida, um atributo que será utilizada para dividir o conjunto de dados e então ir criando os ramos de decisão. Como vimos na seção anterior, vamos utilizar o Ganho de informação para realizar essa decisão. Então, para começar, vamos calcular o ganho de informação para cada um dos atributos, afinal, se quisermos saber qual a melhor, precisamos testar todas. Para calcular o ganho de informação, precisamos primeiro calcular as entropias para os nós filhos que serão gerados caso este atributo seja o selecionado para a divisão. Para isso, aplicamos a divisão considerando o atributo e então, para cada nó gerado, calculamos a entropia. Por exemplo, no caso do atributo Está chovendo?, a divisão gerou dois nós. Um para o valor Sim e outro para o Não. Dentro desses dois nós é feita a inserção dos elementos correspondentes. No caso de quando Está chovendo? for Sim, o único transporte possível é o Ônibus, então, ele é o único inserido no nó resultante. O mesmo vale para quando a resposta é Sim, mas para esta resposta, são possíveis os transportes Ônibus e Caminhada. A Figura 3.15 apresenta a aplicação desta lógica em ambas os atributos (Está chovendo? e Está com tempo?) Figure 3.15: Entropia dos nós filhos - Fonte: Produção do autor Com a entropia contabilizada para cada nó filho, de cada um dos atributos, o ganho de informação pode ser calculado. Vamos então começar calculando a entropia do nó pai, que como visto anteriormente, basicamente representa a entropia da divisão atual dos dados, que neste primeiro momento vai considerar todo o conjunto de dados. Lembre-se de que o cálculo é feito considerando o atributo de classe, neste caso Transporte. \\[\\begin{equation} E_0=-\\left(\\left(\\frac{3}{4}\\cdot log_2\\frac{3}{4}\\right)+\\left(\\frac{1}{4}\\cdot log_2\\frac{1}{4}\\right)\\right)\\:\\:\\approx \\:0.81 \\tag{3.4} \\end{equation}\\] Beleza! Com isso calculado, vejamos o ganho de informação para cada atributo Está chovendo? \\[\\begin{equation} GI\\left(EstaChovendo?\\right)\\:=\\:0.81\\:-\\:\\sum _{n=1}^q\\:\\left(\\frac{N_i}{N}E_i\\right)=0.31 \\tag{3.5} \\end{equation}\\] Está com tempo? \\[\\begin{equation} GI\\left(EstaComTempo?\\right)\\:=\\:0.81\\:-\\:\\sum _{n=1}^q\\:\\left(\\frac{N_i}{N}E_i\\right)=0.31 \\tag{3.4} \\end{equation}\\] Opa! Olha que interessante, no final das contas os dois atributos tem a mesma quantidade de ganho de informação, o que indica que, ao selecionar um ou outro, a qualidade da divisão vai acabar sendo a mesma. Neste caso, outros critérios podem ser aplicados para o desempate, mas esses não serão tratados aqui. O ponto para este exercício é que, com os ganhos de informação calculados pode-se fazer a seleção do atributo para iniciar a divisão dos dados e assim começar a construção da árvore de decisão. Bem, é basicamente assim que a árvore de decisão aprende, nos passos seguintes a este, utilizando a propriedade recursiva, citada anteriormente, estes passos vão sendo aplicados até atingir os critérios de parada já apresentados também. Com isso, você sabe agora o processo base que é utilizado para no processo de treinamento de uma árvore de decisão. Uma vez que a árvore é criada, é possível iniciar o processo de classificação dos dados, onde, as instâncias que precisam ser classificadas são colocadas na estrutura da árvore e então a classe é definida pela aplicação das regras, exatamente como vimos no exemplo de visão geral apresentado. Acho que agora você consegue perceber o motivo deste ser um dos métodos de aprendizado de máquina mais utilizados que existe, sua simplicidade de implementação, interpretação junto a seu alto poder de generalização fazem com que este seja aplicado em diversos casos. 3.2.3 Problemas com overfitting Assim como qualquer outro algoritmo de ML supervisionado, as árvores de decisão podem sofrer com fenômeno de Overfitting e Underfitting. Com um algoritmo de treinamento que trabalha sempre com a melhora em cert, as árvores de decisão, no momento em que estão sendo geradas, podem acabar criando uma quantidade enorme de nós de decisão para minimizar a entropia de seus nós (Aggarwal, n.d.). O contrário disso acaba gerando regras que não são o suficiente para caracterizar os dados e então generalizar as operações. A Figura 3.16 faz a ilustração desses fenômenos considerando a necessidade de identificação de um lenhador. Figure 3.16: Representação do Overfitting e Underfitting - Fonte: Produção do autor Note na Figura 3.16 que, para o caso de Overfitting, uma maior quantidade de regras vai ser gerada, fazendo com que a árvore de decisão aprenda sobre coisas que não são relevantes para a identificação, com o estilo do chapeu do lenhador. Por outro lado, no caso do Overfitting, tem-se que as regras criadas não conseguem capturar as informações úteis dos dados. Para a solução deste problema, pode-se utilizar o processo de Poda, que não será tratado aqui. Caso tenha interesse, Aggarwal (n.d.), listou diversos algoritmos que podem ser utilizados neste processo. Referências Bibliográficas "],
["agrupamento.html", "4 Agrupamento 4.1 O que é um agrupamento? 4.2 Técnicas de agrupamento 4.3 Kmeans 4.4 Agrupamento Hierárquico - Método Aglomerativo", " 4 Agrupamento Afinal, todos nós nos enquadramos em um grupo. - (autor desconhecido) Vimos até agora que os métodos de classificação e regressão usam conjuntos de dados rotulados e usamos tais métodos para encontrarmos modelos que descrevem nossos dados. Em resumo, os métodos de classificação realizam a predição de dados categóricos, de modo oposto, os métodos de regressão fazem a predição de dados contínuos, sendo que ambos precisam à priori dos rótulos para criarem seus modelos. As técnicas de agrupamento operam de modo diferente do que foi visto até então, pois usam conjunto de dados não rotulados, o que modifica a forma com que os métodos aprendem. É isso que vamos ver neste capítulo! Neste capítulo, vamos aprender o que é um agrupamento e suas técnicas, com exemplos práticos usando as linguagens de programação R e Python na plataforma Kaggle. Vamos começar respondendo à pergunta: “O que é um agrupamento?” 4.1 O que é um agrupamento? Para entendermos o conceito de agrupamento, vamos começar com um exemplo prático. Digamos que você tenha um amigo que adora livros, e ele tem muitos deles, mas tem muitas dificuldades em organizá-los. Então, você, uma pessoa muito disposta, sugere ao seu amigo algumas formas de como organizar esses livros, por exemplo: separá-los por gênero, cores ou ordem alfabética. Aderindo às suas dicas, seu amigo decidiu agrupá-los por gênero. Assim, com essa nova organização foi possível encontrar diversos grupos de livros separados por gênero, como apresentado na Figura 4.1. Figure 4.1: Antes e depois da organização da prateleira de livros em grupos baseados em gênero. Utilizamos cores para dividir os gêneros na Figura 4.1 apenas para facilitar a vida do design. A partir do exemplo acima, vimos que os grupos foram criados de acordo com uma característica, neste caso, gênero, mas poderíamos ter grupos de livros organizados com mais de uma característica. Por exemplo, grupos com livros do mesmo gênero e o mesmo ano de lançamento, ou do mesmo gênero e com a mesma cor, e assim por diante. Assim, podemos concluir que livros pertencentes ao mesmo grupo são semelhantes entre si, ou seja, possuem características iguais ou parecidas, mas são diferentes se comparados com livros de outros grupos. Com isso, aprendemos um conceito fundamental em Agrupamento, que é a definição de um grupo. Em aprendizado de máquina, o ato de separar objetos em grupos (em inglês, clusters) por meio de determinadas características de um conjunto de dados é conhecido como agrupamento (em inglês, clustering), e a maneira com que esses grupos são separados é chamado de técnicas ou métodos de agrupamento. De modo oposto do que vimos nos capítulos anteriores, as técnicas de agrupamento possuem o aprendizado não supervisionado, ou seja, usam conjuntos de dados não rotulados para criarem seus modelos. Diante da vasta quantidade de dados gerados diariamente, provindos de diferentes fontes, como radares, redes sociais e smartphones, torna-se cada vez mais difícil obter conjuntos de dados rotulados, pois, por vezes, é necessária a presença de um especialista para criar ou gerenciar esses rótulos. Desta forma, o uso de técnicas de agrupamento pode auxiliar no processo de descoberta de padrões e na criação de novos rótulos a partir de grupos gerados em um agrupamento. Técnicas de agrupamento caem como luva na mão de cientistas e analista de dados, uma vez que identificam padrões em conjuntos de dados através da organização dos dados em grupos. Em detalhes, os grupos são formados por objetos que possuem a máxima semelhança entre si, e a mínima semelhança com elementos de outros grupos (Aghabozorgi, Shirkhorshidi, and Wah 2015) - como vimos no exemplo dos livros. Para identificar esse grau de semelhança entre os objetos precisamos de uma medida de similaridade, pois é ela que vai nos dizer o quão parecidos ou não são os objetos que desejamos agrupar. Para isto, medidas de distância são usadas para determinar tal similaridade, por exemplo, a distância euclidiana entre os pontos de dois objetos. Explicamos a diferença entre cientista e analista de dados no nosso minicurso de Introdução à análise de dados. Na Figura 4.2 vemos dois exemplos de agrupamentos em um espaço com duas dimensões (\\(X\\) e \\(Y\\)). O primeiro agrupamento 4.2(a) possui 3 grupos e o segundo 8 4.2(b). Note que o termo objeto refere-se a uma linha do conjunto de dados, conforme ilustrado, em que um objeto no Grupo 8 possui os valores de \\(X = 8\\) e \\(Y = 8\\). Figure 4.2: Exemplo de dois agrupamentos em um plano com duas dimensões (X,Y). Observam-se 3 grupos em a) e 8 grupos em b). Adaptado de Esling and Agon (2012) De acordo com Jain (2010), um dos grandes estudiosos da área de aprendizado de máquina, o uso de técnicas agrupamento podem ser divididos em três principais aplicações: Descoberta de conhecimento em estruturas intrínsecas: para obter informações sobre dados, gerar hipóteses, detectar anomalias e identificar características salientes. Classificação natural: por exemplo, na Biologia, para identificar o grau de semelhança entre formas ou organismos (relação filogenética). Compressão: como um método para organizar os dados e resumi-los através de protótipos de agregados. Além das aplicações mencionadas por Jain (2010), como descoberta de padrões, detecção de outliers e análise de distribuições intrínsecas nos dados; segundo Han, Pei, and Kamber (2011), às técnicas de agrupamento podem ser usadas em etapas de pré-processamento para outros algoritmos, por exemplo, na caracterização, seleção de subconjuntos de atributos e classificação. Desta forma, diante dessas aplicações, o uso de técnicas de agrupamento encontra-se em diversas áreas do conhecimento, entre elas: Biologia: (Gasch and Eisen 2002), Sensoriamento Remoto: (He et al. 2014) e business intelligence: (Wang, Wu, and Zhang 2005). Agora que entendemos o que é um agrupamento e suas aplicações, na próxima subseção vamos mostrar algumas técnicas de agrupamento e quais as principais diferenças entre elas. 4.2 Técnicas de agrupamento Como vimos anteriormente, cada técnica de agrupamento possui uma abordagem para criar grupos em um conjunto de dados, por exemplo, algumas técnicas são baseadas em pontos centrais de cada grupo (centróide); em outras os grupos são criados seguindo uma hierarquia presente nos dados. Neste minicurso, vamos mostrar duas abordagens de técnicas de agrupamento: baseadas em partição e hierarquia. A seguir vamos comentar de maneira geral cada uma. 4.2.1 Técnicas baseadas em Partição Nas técnicas baseadas em partição cada grupo representa uma partição, sendo que o número de grupos é definido pelo usuário. Desta forma, cada partição deve conter pelo menos um objeto, e cada objeto deve pertencer somente a um grupo, o que é conhecido como separação exclusiva de grupos (em inglês, exclusive cluster separation ou hard cluster). Existem métodos que flexibilizam o critério de separação exclusiva, sendo assim, o objeto possui porcentagens de pertencimento a cada grupo, o que é conhecimento como técnicas de agrupamento nebulosas (em inglês, fuzzy clustering). A Figura 4.3 mostra um exemplo de agrupamento baseado em partição, as linhas pretas demarcam cada partição. Figure 4.3: Exemplo de um agrupamento baseado em partição. As linhas pretas demarcam as partições e as formas geométricas preenchidas os centróides. Adaptado de Developers (2020) Além das características mencionadas acima, grande parte das técnicas baseadas em partição usam medidas de distâncias para determinar o grau de similaridade de cada grupo. Outra característica importante de mencionar é a representação de cada grupo, o centróide (Figura 4.3), que pode ser definido pela média ou qualquer outra medida estatística. Um dos algoritmos mais utilizados baseados em partição é o Kmeans - vamos explicá-lo nas próximas páginas. 4.2.2 Técnicas baseadas em Hierarquia Nas técnicas baseadas em hierarquia, diferente do que vimos nas técnicas de partição, não é requerido que o especialista especifique o número de grupos, pois seu algoritmo busca por correlações entre os objetos do conjunto de dados para criar os grupos. Baseado no modo em que sua decomposição hierárquica é formada, os métodos hierárquicos podem ser divididos em aglomerativos ou divisivos. No método aglomerativo, também conhecido como AGNES (do inglês, Agglomerative Nesting), considera-se, inicialmente, que cada objeto seja seu próprio grupo, e a cada iteração, os dois grupos mais parecidos são unidos, assim, formando um novo um grupo. Esse processo é repetido até que todos os objetos estejam em um único grupo. De modo oposto, o método divisivo, também conhecido como DIANA (do inglês, Divise Analysis), considera-se que todos objetos pertencem a um único grupo, e posteriormente, o único grupo é dividido até obter-se um grupo para cada objeto ou até que um critério de párada seja atendido (Han, Pei, and Kamber 2011). Figure 4.4: Exemplo do processo de criação de grupos baseado nas técnicas de agrupamento hierarquicas. - Fonte: Han, Pei, and Kamber (2011) Na Figura 4.4 é possível observar o processo de criação de grupos dos dois métodos mencionados, AGNES e DIANA, com base em um conjunto de dados com cinco objetos \\(a, b, c, d, e\\). Por exemplo, no método aglomerativo, o objeto \\(a\\) junta-se com o objeto \\(b\\) no mesmo grupo, pois possuem maior semelhança se comparados com outros objetos, o mesmo processo acontece com os objetos \\(d\\) e \\(e\\). Por outro lado, o método divisivo inicia-se com todos os objetos pertencendo a um grupo, e a cada passo vai se dividindo com base na semelhança dos objetos dentro do grupo. Uma técnica que auxilia na interpretação dos agrupamentos hierárquicos é o dendrograma, apresentado na Figura 4.5, que faz representação em forma de árvore, nele é possível obter informações sobre a estrutura do agrupamento realizado. No nível 0 é possível observar os objetos em seus próprios grupos, e, conforme o nível da árvore aumenta em relação a sua raiz, menor são as similaridades entre os objetos do mesmo grupo (Han, Pei, and Kamber 2011). Figure 4.5: Exemplo de representação do agrupamento hierárquico. - Fonte: Han, Pei, and Kamber (2011) Neste minicurso, vamos abordar o método aglomerativo, no entanto, os mesmos conceitos podem ser aplicados no método divisivo. 4.3 Kmeans Kmeans ou K-médias é uma técnica de agrupamento que usa o método de partição para dividir o conjunto de dados em \\(k\\) grupos, em que o valor de \\(k\\) é definido pelo usuário. De forma geral, o algoritmo do Kmeans visa diminuir a variação intra-grupos, ou seja, criar grupos em que os objetos sejam semelhantes entre si. Então, para que esse objetivo seja atendido, em cada iteração os centróide de cada grupo são atualizados para refinar a qualidade dos grupos. Posteriormente vamos apresentar algumas heurísticas que auxiliam na determinação do número de grupos Para exemplificar o funcionamento do Kmeans, vamos usar o conjunto de dados apresentado na Figura 4.6, no qual é possível observar 13 objetos em um plano cartesiano (X,Y). Figure 4.6: Conjunto de dados utilizado como exemplo. Para separar as observações de forma a obter grupos mais homogêneos, a técnica em questão usa o conceito de centróide, em que é definido um ponto central para cada grupo com base em medidas estatísticas, como a média, e a cada iteração o ponto central é atualizado até que o critério de párada seja alcançado. Então, o primeiro passo do algoritmo é a escolha da quantidade de grupos, neste exemplo, escolhemos três grupos. Com isso, a próxima etapa consiste em sortear, de forma aleatória, o centróides de cada grupo, como apresentado na Figura 4.7. Após o sorteio, os objetos são atribuidos aos seus respectivos grupos, como também apresentado na Figura 4.7. A atribuição é feita com base em uma medida de distância, geralmente, euclidiana; cada objeto é comparado com cada centróide, assim, o objeto é atribuído ao grupo em que possui a menor distância com seu centróide. Figure 4.7: Exemplo de funcionamento do algoritmo Kmeans. Na Figura 4.7 observamos três centróides \\(c_1\\), \\(c_2\\) e \\(c_3\\), nas posições: \\((1.5,1.3)\\), \\((4.3,1.7)\\) e \\((2.7,3.3)\\), respectivamente. Para sabermos em qual grupo o objeto será atribuído basta usar uma medida de distância e verificar qual centróide está mais próximo deste objeto. Por exemplo, seja o objeto que está na posição \\((3.2,1)\\), temos: \\[ x_{obj} = 3.2\\\\ y_{obj} = 1.0\\\\ \\\\ x_{c1} = 1.5\\\\ y_{c1} = 1.3\\\\ \\\\ x_{c2} = 4.3\\\\ y_{c2} = 1.7\\\\ \\\\ x_{c3} = 2.7\\\\ y_{c3} = 3.3\\\\ \\\\ dist_{c1} = \\sqrt{(x_{obj} - x_{c1})^2 + (y_{obj} - y_{c1})^2} \\approx 1.73 \\\\ dist_{c2} = \\sqrt{(x_{obj} - x_{c2})^2 + (y_{obj} - y_{c2})^2} \\approx 1.30 \\\\ dist_{c3} = \\sqrt{(x_{obj} - x_{c3})^2 + (y_{obj} - y_{c3})^2} \\approx 2.35\\\\ \\] Desta forma, a menor distância (\\(dist\\)) é dada pelo centróide \\(c_2\\), então o objeto é atribuído ao grupo deste centróide. Após a atribuição, o centróide é recalculado com base nos objetos do grupo (Figura 4.8). Como foi dito anteriormente, podemos usar medidas estatísticas, como a média para achar a nova posição do centróide. Caso, não haja alterações nos valores dos centróides, o algoritmo pára. Caso não, o processo se repete: os centróides são atualizados e os objetos são atribuídos a cada grupo. Figure 4.8: Exemplo de funcionamento do algoritmo Kmeans. Resumidamente, podemos descrever o algoritmo do Kmeans conforme as etapas abaixo: (1º Passo) Definição da quantidade de grupos; (2º Passo) Sorteio de centróides para cada grupo; (3º Passo) Atribuição dos objetos a cada grupo; (4º Passo) Atualização dos centróides de cada grupo; (5º Passo) Caso os centróides sejam atualizados, volte ao passo (3), caso não, o algoritmo pára. Lembrando que, outro critério de párada é o número de iterações, pois nem sempre é possível obter grupos homogêneos com poucas iterações. Ainda mais se considerarmos conjuntos de dados de aplicações do mundo real. Outro fato a mencionar, é o sorteio de centróides, pois existem diversas métricas que podem ser utilizadas em relação a inicialização dos centróides, por exemplo, k-means++. Agora que entendemos mais de perto como o Kmeans funciona. Talvez, você esteja se perguntando: “mas, como verificamos a eficiência do nosso agrupamento?” Isso é o que vamos responder na próxima subseção. Também vamos comentar como podemos estimar o número de grupos em um conjunto de dados. 4.3.1 Como avaliar o Kmeans? Como mencionado anteriormente, o objetivo do kmeans é minimizar a variação intra-grupos, vimos que isso é feito com base na atualização dos centróides em cada iteração do algoritmo, até que não haja mudança na atualização dos centróides. Então, para verificarmos o erro de cada grupo, podemos usar a soma das diferenças quadráticas (do ingles, sum of squared error) entre cada objeto e o centróide do seu grupo. Por exemplo, na Figura 4.8, para calcularmos o erro quadrático do Grupo 1, temos a seguinte equação: \\[ SSE(grupo_{1}) = \\sum_{x_i \\in grupo_{1}} dist(x_i - c_1)^2 \\] Em que \\(x_i\\) representa cada objeto do Grupo 1, \\(c_1\\) representa o centróide deste mesmo grupo e \\(dist\\) a medida de distância utilizada. Logo, o \\(SSE(grupo_1) \\approx 1.9\\), em outras palavras, o erro quadrático do Grupo 1 é a soma da distância de todos os objetos em relação seu centróide. Então, para calcularmos o erro total dos grupos, basta fazermos um somatório de cada SSE: \\[ SSE_{total} = SSE(grupo_1) + SSE(grupo_2) + SSE(grupo_3) \\] Desta forma, para este agrupamento apresentado no exemplo acima, temos o \\(SSE_{total} \\approx x\\), em outras palavras, o erro total do nosso agrupamento, é representado pela soma dos erros quadráticos de cada grupo. Agora que vimos como calcular o erro do nosso agrupamento, só nos resta uma forma de “descobrir” qual a quantidade ideal de grupos para cada agrupamento e é que vamos ver na próxima subseção. 4.3.2 Como definir a quantidade de grupos? Nesta subseção vamos mostrar dois métodos para definir a quantidade de grupos, sendo eles: Método do Cotovelo e Método da Silhueta. É recomendado perguntar ao especialista sobre a quantidade de grupos, no caso da ausência de um, podemos optar pelo uso de heurísticas. Duas delas são apresentadas abaixo. 4.3.2.1 Método do cotovelo O método do cotovelo (do inglês, elbow) usa o somatório do erro total dos grupos em cada agrupamento para determinar o número de K, como apresentado na Figura 4.9. Então, a ideia deste método é gerar diversos agrupamentos com diferentes números de grupos, com o incremento do número de grupos, o erro total tende a diminuir, até que o erro se aproxime de zero, que é quando terá um objeto por grupo. Figure 4.9: Método do cotovelo aplicado no conjunto de dados de exemplo. De acordo com o método, o número ideal de grupos se dá quando o ponto forma uma curva semelhante de um cotovelo, que é o ponto considerado ideal. Pois os pontos acima do cotovelo estão em uma região de underfitting e abaixo dele em overfitting. 4.3.2.2 Método de Silhueta O método de Silhueta determina o quão bem cada objeto está alocado em um grupo, ou seja, a homogeneidade de um grupo. O índice de Silhueta varia de -1 a 1. Valores próximos a 1 indicam que o objeto possui semelhança com objetos de seu grupo e dessemelhança com objetos de outros grupos. Figure 4.10: Método de Silhueta aplicado no conjunto de dados de exemplo. A Figura 4.10 mostra um exemplo de aplicação do método de Silhueta, o número ideal de grupos é escolhido com base na maior média do índice de Silhueta, ou seja, neste exemplo apresentado acima, com 3 grupos foi possível obter grupos com objetos semelhantes entre si, e dissemelhante se comparados com objetos de outros grupos. Agora que entendemos os fundamentos da técnica de agrupamento Kmeans e algumas técnicas de estimativas de grupos, vamos apresentar alguns exemplos práticos na plataforma kaggle, os exemplos estão disponíveis nas linguagens R e Python. 4.4 Agrupamento Hierárquico - Método Aglomerativo Os métodos de agrupamento hierárquicos que pertencem à categoria de aglomerativos baseiam-se na junção de grupos até que apenas um grupo englobe todos os outros, esse chamado de raiz. No método aglomerativo, os objetos são comparadas com base em uma medida de distância, assim como vimos no Kmeans, em que foi usado a distância euclidiana. No entanto, neste método, os grupos também são comparados, para que a união entre eles seja realizada. Para compararmos a similaridade entre cada grupo usamos as medidas de ligação (do inglês, linkage measures). As medidas de ligação mais conhecidos são: Ligação completa ou distância máxima: Em inglês, complete linkage, determina como distância entre dois grupos os pares de objetos mais distantes. \\[dist_{max}(g_1, g_2) = \\underset{obj_{g_1} \\in g_1,obj_{g_2} \\in g_2}{\\max \\{|obj_1 - obj_2|\\}}\\] Em que \\(g_1\\) e \\(g_2\\) correspondem aos grupos 1 e 2, respectivamente. Ligação única ou distância mínima: Em inglês, single linkage, determina como distância entre dois grupos os pares de objetos mais próximos. \\[dist_{min}(g_1, g_2) = \\underset{obj_{g_1} \\in g_1,obj_{g_2} \\in g_2}{\\min \\{|obj_1 - obj_2|\\}}\\] Distância baseada em média: Em inglês, average method, determina como distância entre dois grupos a média das distâncias entre os pares objetos. \\[dist_{avg} = \\frac{1}{|g_1||g_2|} \\sum_{i = 1}^{g_1}\\sum_{j=1}^{g_2}dist(obj_i, obj_j)\\] Para exemplificar o funcionamento do método aglomerativo, vamos usar o conjunto de dados apresentado na Figura 4.11, no qual é possível observar 6 objetos (A,B,C,E,F) em um plano cartesiano (X,Y). Os objetos foram marcados para facilitar a visualização do dendograma. Figure 4.11: Conjunto de dados utilizado como exemplo. Como foi mencionado, os métodos aglomerativos iniciam de forma em que cada objeto seja seu próprio grupo, e, com isso, a cada iteração os grupos vão se juntando e combinando os objetos dentro deles. O primeiro passo dos algoritmos hierárquicos, abrangendo às duas categorias, divisivos e aglomerativo, é calcular a matriz de distância entre todos os objetos. A Figura 4.12 mostra um exemplo do cálculo da distância do objeto A entre os outros. Figure 4.12: Conjunto de dados utilizado como exemplo. Repare que é necessário calcular as distâncias entre os outros objetos: \\(B -&gt; C, B -&gt; D, B -&gt; E, B -&gt; F\\); \\(C -&gt; D, C -&gt; E, C -&gt; F\\); \\(D -&gt; E, D -&gt; F\\) e \\(E -&gt; F\\). Vamos deixar como tarefa para que você calcule a distância euclidiana entre os outros objetos. Então, seguindo com o algoritmo do método aglomerativo, o próximo passo consiste na junção dos objetos em grupos, como foi dito, cada objeto inicialmente pertence ao seu próprio grupo. A etapa de junção dos objetos aos grupos segue a mesma ideia que vimos no Kmeans, só que aqui estamos comparando dois objetos, e não os objetos com seus centróides. Então, os objetos mais próximos se juntam em um grupo, como apresentado na Figura 4.13. A junção segue uma ordem, os objetos com as menores distâncias se juntam primeiro, então, neste exemplo, primeiro vamos juntar os objetos E e F que estão em 1 unidade de distância, depois juntamos os objetos A e B, como mostrado na Figura 4.12, que possuem a distância de \\(1.41\\), e, por fim, juntamos os objetos D e C que possuem a distância de \\(1.5\\). Utilizando o Dendrograma é possível observar essas distância no eixo de Altura. Figure 4.13: Etapa de junção dos objetos. Depois de juntarmos todos os pares de objetos em um grupo, que até então estavam em seus próprios grupos, é hora de juntarmos os grupos. Nesta etapa de junção dos grupos, vamos utilizar as abordagens de ligação que foram mencionadas acima. A Figura 4.14 apresenta o funcionamento dos métodos de ligação mencionados, em A) temos a ligação completa, que visa juntar os dois grupos mais próximos pelos objetos mais distantes; em B) temos a ligação única, que faz a junção de modo oposto do que vimos em A), usando os objetos mais próximos de cada grupo, e em C) temos a ligação pela média, que visa realizar a junção por meio da média das distâncias entre cada objeto dos dois grupos. Figure 4.14: Etapa de junção dos grupos. Por fim, a Figura 4.15 apresenta o resultado final do agrupamento aglomerativo por meio do método de ligação completa, veja que ao final apenas um grupo engloba todo os objetos. Na junção do grupo que engloba os objetos A, B, C e D com o grupo dos objetos E e F, a distância máxima (4.16) se dá pelos objetos A e F. Vamos deixar como tarefa para que você monte os dendrogramas dos métodos de ligação única e por média. Lembrando que o dendrograma mostra a distância entre os grupos por meio do eixo de altura. Desta forma, o dendrograma varia com o método de ligação. Por exemplo, o dendrograma do método de ligação única é mais achatado, visto que a maior altura corresponde a 2.06. Figure 4.15: Etapa de junção dos grupos - Parte 2. Resumidamente, podemos descrever o algoritmo do método aglomerativo conforme as etapas abaixo: (1º Passo) Crie a matriz de distância entre os objetos; (2º Passo) Faça com que cada objeto seja um grupo; (3º Passo) Junte os grupos mais próximos; (4º Passo) Atualize a matriz de distância; (5º Passo) Repita o passo 3 e 4 até que apenas um grupo englobe todos os objetos. Agora que entendemos como funciona o método aglomerativo, você deve estar se perguntando: “Qual método de ligação devo usar?” e é isso é o que vamos ver agora. 4.4.1 Qual método de ligação deve ser usado? Assim como a escolha do número de grupos no Kmeans, a escolha do método de ligação também é essencial, no entanto, cabe ao usuário testar os diversos métodos de ligação para determinar qual se adequa melhor ao seu caso. Vamos listar algumas vantagens e desvantagens dos métodos mencionados acima com base do material dos professores Gao and Zhang (2012). Método Vantagens Desvantagens Ligação única Consegue lidar com formas não elípticas Sensível a ruídos e outliers Ligação Completa Consegue lidar melhor com ruídos e outliers Tende a quebrar grandes grupos; Possui viés para agrupamentos com dados circulares. Distância baseada em média Consegue lidar melhor com ruídos e outliers Possui viés para agrupamentos com dados circulares. Além dos métodos de ligação mencionados acima, existem outros que são igualmentes importantes, como: Ward e Centróide 4.4.2 Como visualizar os grupos no dendrograma? Como vimos, o dendrograma mostra a hierarquia do nosso agrupamento com o formato de árvores, aos que estão familiarizados com estrutura de dados, uma árvore binária. Assim, podemos fazer um corte nessa árvore e selecionar os grupos que desejamos de acordo com a altura especificada. Por exemplo, a Figura 4.16 mostra a aplicação do método aglomerativo por meio de ligação completa no conjunto de dados utilizado no exemplo do Kmeans, os \\(K\\)’s em cada dendrograma é o nível em que o corte foi realizado. Desta forma, cortando um nível abaixo da raiz, ou seja, \\(k = 2\\), obteve-se dois grupos, com \\(k = 5\\), obteve-se cinco grupos, e assim por diante. O corte no dendrograma nos dá uma visão dos grupos intrínsecos em cada nó da árvore, o que pode auxiliar no entendimento do conjunto de dados Figure 4.16: Cortes nos dendrogramas para visualização dos grupos. Como mostrado acima, o \\(k\\) define a quantidade de grupos que serão mostrados no dendrograma através de um corte feito numa árvore binária. E como foi visto no Kmeans utilizamos as heurísticas do cotovelo e silhueta para determinar o número de \\(k\\), o que pode ser feito neste método também. Você deve estar se perguntando: “Como faço para avaliar os grupos gerados no corte?”. Para avaliarmos a qualidade dos grupos podemos usar o índice de Silhueta, que verifica o quão bem os objetos foram ajustados em um grupo e quão eles se diferenciam dos objetos de outros grupos. Agora que entendemos os fundamentos da técnica de agrupamento aglomerativa e como podemos definir os grupos, vamos apresentar alguns exemplos práticos na plataforma kaggle, os exemplos estão disponíveis nas linguagens R e Python. Referências Bibliográficas "],
["exemplos.html", "5 Exemplos", " 5 Exemplos Esta seção mostra exemplos de aplicação de cada um dos algoritmos apresentados neste livro-texto. Todos os exemplos foram feitos utilizando a linguagem de programação Python com o auxílio da plataforma Kaggle. Os links para cada um dos exemplos são listados abaixo. Caso tenha interesse em conhecer um pouco melhor a plataforma Kaggle, consulte os apêncides desse material, lá criamos um passo a passo sobre o uso do Kaggle. Links dos exemplos de Regressão Exemplo de uso de Regressão Linear Exemplo de uso da SVR Links dos exemplos de Classificação Exemplo de uso do kNN Exemplo de uso da Árvore de Decisão Links dos exemplos de Agrupamento Exemplo de uso do K-Means Exemplo de uso do método hierarquico "],
["considerações-finais.html", "6 Considerações finais", " 6 Considerações finais Lorem ipsum dolor sit amet "],
["sobre-o-kaggle.html", "7 Sobre o Kaggle 7.1 Cadastro 7.2 Criação de um notebook", " 7 Sobre o Kaggle Criada em 2010, Kaggle é uma plataforma que possibilita a realização de uma ampla quantidade de atividades que envolvem as áreas de Data Science e Machine Learning. Com o uso da plataforma, o usuário tem acesso a um ambiente web gratuito para a execução de código nas linguagens Python e R. Neste ambiente são disponibilizados diversos conjuntos de dados que podem ser fácilmente importados para os ambientes de análise. Além disso, o Kaggle é muito conhecido por ter uma grande comunidade de usuários ativos. Outra característica que torna o Kaggle muito famoso são as competições, nessas, os usuários são desafiados a resolver problemas do mundo real. Normalmente empresas e instituições de ensino utilizam o Kaggle para criar tais competições e utilizar essas como um catalizador para a identificação de bons profissionais. Dado este contexto inicial, neste documento, serão abordados os primeiros passos de uso da plataforma e os principais conceitos envolvidos em seu uso. 7.1 Cadastro Para iniciar o processo de cadastro, acesse o Kaggle. Na página principal da plataforma, apresentada na Figura 7.1, clique no ícone \\({\\textbf{Register}}^{\\color{red}1 \\ ou \\ \\color{red}3}\\). Caso já tenha uma conta registrada, utilize a opção \\({\\textbf{Sign In}}^{\\color{red}{2}}\\). Figure 7.1: Tela inicial Após aceitar os termos de uso, é necessário inserir o código de segurança (Figura 7.2), o qual foi enviado no email cadastrado. Caso não tenha recebido, verifique a caixa de Spam. Figure 7.2: Tela de segurança 7.2 Criação de um notebook Dentro da plataforma Kaggle, os ambientes para a execução dos códigos, chamados de Notebooks, são criados com o Jupyter Notebook, uma ferramenta que possibilita a criação de documentos interativos, com códigos que podem ser executados e misturados com equações, visualização de dados e textos descritivos. Para criar um novo notebook, após o ingresso na plataforma, clique em \\({\\textbf{Notebooks}}^{\\color{red}1}\\), como apresentado na Figura 7.3, em seguida clique em \\({\\textbf{New Notebook}}^{\\color{red}2}\\). Caso tenha dúvidas sobre a plataforma e queira saber mais detalhes, recomendamos a leitura da documentação Figure 7.3: Criação de um novo notebook Na tela de configurações de um novo notebook, presente na Figura 7.4, é possível selecionar entre duas linguagens de programação\\({\\textbf{}}^{\\color{red}1}\\), R e Python, assim como, o tipo de ambiente que deseja criar\\({\\textbf{}}^{\\color{red}2}\\), neste curso o Notebook. O ambiente disponibiliza acesso a GPUs\\({\\textbf{}}^{\\color{red}3}\\) e sincronização com os serviços Google Cloud, tais configurações não serão utilizadas neste curso. Figure 7.4: Criação de um novo notebook Após selecionada as configurações do Notebook (Figura 7.4), o ambiente levará alguns segundos para iniciar, a Figura apresenta 7.5 o ambiente em questão. O Notebook criado é composto por células, as quais são destinadas para escrever códigos e executá-los de modo interativo. É possível criar uma nova célula, assim como executá-la nos botões apresentados no canto superior direito\\({\\textbf{}}^{\\color{red}1}\\), para apagar ou mover\\({\\textbf{}}^{\\color{red}5}\\) a célula criada basta acessar os ícones que aparecem no canto direito ao clicar na célula. A plataforma oferece 4.9GB de espaço para armazenamento de dados\\({\\textbf{}}^{\\color{red}3}\\), 16GB de mémoria e uso de CPU por até 9 horas. Outro recurso interessante disponibilizado pela plataforma, é a possibilidade de versionar\\({\\textbf{}}^{\\color{red}3}\\) o notebook, de forma a garantir diferentes versões do notebook no mesmo ambiente. Figure 7.5: Detalhes do ambiente Isso é só o começo, existem diversas outras informações e possibilidades na plataforma, como por exemplo a disponibilização de cursos online e gratuito para o aprendizado de Data Science. Para se aprofundar, acesse a Documentação do Kaggle "],
["referências-bibliográficas.html", "8 Referências Bibliográficas", " 8 Referências Bibliográficas "]
]
